{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(th.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des données et construction des train, dev et test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##On transforme les 3 colonnes en 1 : url, titre et le texte deviennent un seul élément\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, encoding=\"utf8\", newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            txt = clean_str(row[2])\n",
    "            txt = \" \".join(txt.split(\"\\n\"))\n",
    "#             d = clean_str(row[0])+\" \"+clean_str(row[1])\n",
    "            labels.append(row[3])\n",
    "            data.append(txt)    \n",
    "    data.pop(0)\n",
    "    labels.pop(0)\n",
    "    labels = [float(l) for l in labels]\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_data(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image copyright getty images on sunday morning , donald trump went off on a twitter tirade against a member of his own party this , in itself , is n't exactly huge news it 's far from the first time the president has turned his rhetorical cannons on his own ranks this time , however , his attacks were particularly biting and personal he essentially called tennessee senator bob corker , the chair of the powerful senate foreign relations committee , a coward for not running for re election he said mr corker begged for the president 's endorsement , which he refused to give he wrongly claimed that mr corker 's support of the iranian nuclear agreement was his only political accomplishment unlike some of his colleagues , mr corker free from having to worry about his immediate political future did n't hold his tongue skip twitter post by senbobcorker it 's a shame the white house has become an adult day care center someone obviously missed their shift this morning senator bob corker \\( senbobcorker \\) october 8 , 2017 report that was n't the end of it , though he then spoke with the new york times and really let the president have it here are four choice quotes from the tennessee senator 's interview with the times and why they are particularly damning i do n't know why the president tweets out things that are not true you know he does it , everyone knows he does it , but he does you ca n't really sugarcoat this one mr corker is flat out saying the president is a liar and everyone knows it the senator , in particular , is challenging mr trump 's insistence that he unsuccessfully pleaded for his endorsement , but the accusation is much broader mr corker and the president used to be something akin to allies the tennessean was on mr trump 's short list for vice president and secretary of state image copyright getty images image caption bob corker at trump campaign rally in july 2016 those days are seemingly very much over now and it 's not like mr corker is going anywhere anytime soon although he 's not running for re election , he 'll be in the senate , chairing a powerful committee , until january 2019 the president 's margin for success in that chamber is razor thin if democrats can continue to stand together in opposition , he can afford to lose only two votes out of 52 republican senators that 's why healthcare reform collapsed in july and it could be bad news for tax efforts from here on out , mr corker is n't going to do the president any favours look , except for a few people , the vast majority of our caucus understands what we 're dealing with here frustration in congress has been growing over what republicans feel has been the president 's inability to focus on advancing their agenda getting a sharply divided party to come together on plans to repeal obamacare , reform taxes or boost infrastructure spending is challenging enough doing so when the president stirs up unrelated controversies on a seemingly daily basis makes things all the harder one of the president 's gifts has been his ability to shake off negative stories by quickly moving on to a different subject that worked brilliantly during his presidential campaign , but it 's less effective during the legislative slow grind image copyright getty images image caption corker at the confirmation hearing for secretary of state rex tillerson for months , republicans in congress have been grumbling about this in the background and among themselves occasionally , someone like mr mcconnell will lament that the president does n't understand how the senate works mr corker has now stated it loud and clear and , what 's more , he says almost everyone agrees with him they 've kept silent until now because they still hope to pass conservative legislation that the president can sign or fear mr trump 's legions will back a primary challenge next year or stay home during the general election if that calculus ever changes if it becomes riskier to stay silent than speak out mr trump will be in real trouble a lot of people think that there is some kind of 'good cop , bad cop' act underway , but that 's just not true time and again , mr trump has appeared to undercut secretary of state rex tillerson and others in his administration who are attempting to use soft diplomacy to deal with a range of international crises the war against the taliban in afghanistan , iran 's compliance with the multinational nuclear agreement , the ongoing dispute between qatar and its persian gulf neighbours , the unrest in venezuela and , most recently , north korea 's continued ballistic missile tests have all been the target of the president 's offhand remarks and twitter invective some administration defenders have said this is all a part of mr trump 's strategy an updated version of the nixon era madman theory , in which the president forces adversaries to give way because they fear an unpredictable us leader 's actions mr corker is n't buying it there 's no strategy , he says , just the possibility of chaos which he hopes mr trump 's senior advisers will be able to avoid i know for a fact that every single day at the white house , it 's a situation of trying to contain him there 's now a growing collection of john kelly face palm photos that serve as a testament to the chief of staff 's reported frustration at dealing with the president mr trump goes off script to praise torch bearing white nationalists at a rally in charlottesville , and mr kelly is captured closing his eyes and rubbing the arch of his nose , as if attempting to stave off a migraine image copyright reuters image caption white house chief of staff john kelly looks on as us president donald trump speaks at a campaign rally the president calls north korean leaders criminals in a speech to the united nations , and mr kelly straight up buries his face in his hands the white house communications team is often left scrambling to try to explain or reframe an indelicate presidential joke or remark that directly contradicts what was until then the official administration line even though mr kelly has brought some discipline to the west wing staff , the president still marches to the beat of his own drum and continues to have unfettered access to his phone 's twitter app bob corker is only the latest person politician , journalist , sports star or celebrity to feel the mercurial president 's uncontainable ire\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4009,)\n",
      "Nombre de fake news : 2137\n",
      "Nombre de news correctes : 1872\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "fake_data = data[labels==0]\n",
    "real_data = data[labels==1]\n",
    "fake_labels = labels[labels==0]\n",
    "real_labels = labels[labels==1]\n",
    "print(\"Nombre de fake news : {}\".format(len(fake_data)))\n",
    "print(\"Nombre de news correctes : {}\".format(len(real_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2137,)\n"
     ]
    }
   ],
   "source": [
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([*real_data[:int(len(real_data)*0.8)],*fake_data[:int(len(fake_data)*0.8)]])\n",
    "train_labels = np.array([*real_labels[:int(len(real_labels)*0.8)],*fake_labels[:int(len(fake_labels)*0.8)]], dtype=np.float32)\n",
    "\n",
    "dev_data = np.array([*real_data[int(len(real_data)*0.8):int(len(real_data)*0.9)], *fake_data[int(len(fake_data)*0.8):int(len(fake_data)*0.9)]])\n",
    "dev_labels = np.array([*real_labels[int(len(real_labels)*0.8):int(len(real_labels)*0.9)], *fake_labels[int(len(fake_labels)*0.8):int(len(fake_labels)*0.9)]], dtype=np.float32)\n",
    "\n",
    "test_data = np.array([*real_data[int(len(real_data)*0.9):], *fake_data[int(len(fake_data)*0.9):]])\n",
    "test_labels = np.array([*real_labels[int(len(real_labels)*0.9):], *fake_labels[int(len(fake_labels)*0.9):]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du train set : 3206\n",
      "taille du dev set : 401\n",
      "taille du test set : 402\n"
     ]
    }
   ],
   "source": [
    "print(\"taille du train set : {}\".format(len(train_data)))\n",
    "print(\"taille du dev set : {}\".format(len(dev_data)))\n",
    "print(\"taille du test set : {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  43464\n"
     ]
    }
   ],
   "source": [
    "txt = [w for txt in train_data for w in txt.split()]\n",
    "          \n",
    "# token = nltk.word_tokenize(txt)\n",
    "words = Counter(txt)\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['_PAD','_UNK'] + words\n",
    "vocab_size = len(words)\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "word2idx[\"UNK\"]=len(word2idx)\n",
    "print(\"vocab size : \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[word2idx[w] for w in news.split()] for news in train_data]\n",
    "dev_data = [[word2idx[w] if w in word2idx else 0 for w in news.split()] for news in dev_data]\n",
    "test_data = [[word2idx[w] if w in word2idx else 0 for w in news.split()] for news in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input(data, seq_len):\n",
    "    features = np.zeros((len(data), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(data):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "## on pad les input pour pouvoir créer des datasets pytorch et simplifier l'apprentissage (fonction de train plus claire, batch plus facile, etc)\n",
    "on peut tester plusieurs taille de padding : input le plus grand, moyen, min, ou arbitraire du genre 200, 500, etc. On peut aussi ainsi réduire le temps de train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seq_len = max([len(s) for s in train_data])\n",
    "# dev_seq_len = max([len(s) for s in dev_data])\n",
    "# test_seq_len = max([len(s) for s in test_data])\n",
    "\n",
    "# train_data = pad_input(train_data, train_seq_len)\n",
    "# dev_data = pad_input(dev_data, dev_seq_len)\n",
    "# test_data = pad_input(test_data, test_seq_len)\n",
    "\n",
    "seq_len = 200\n",
    "\n",
    "\n",
    "train_data = pad_input(train_data, seq_len)\n",
    "dev_data = pad_input(dev_data, seq_len)\n",
    "test_data = pad_input(test_data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(th.from_numpy(train_data).type(th.LongTensor), th.from_numpy(train_labels))\n",
    "dev_data = TensorDataset(th.from_numpy(dev_data).type(th.LongTensor), th.from_numpy(dev_labels))\n",
    "test_data = TensorDataset(th.from_numpy(test_data).type(th.LongTensor), th.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(L_CBOW_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.l1 = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs = th.sum(inputs, dim=1)\n",
    "        out = th.sigmoid(self.l1(inputs))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(C_CBOW_classifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv1d(20, 50, 5, 1)\n",
    "        self.dense1 = nn.Linear(450, 250)\n",
    "        self.dense2 = nn.Linear(250, 64)\n",
    "        self.dense3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embeddings(x)\n",
    "        x = th.sum(x, dim=0)\n",
    "        x = x.view(1, 1,-1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2, 2)\n",
    "        x = x.view(-1, 450)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        return th.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lr = 1e-2\n",
    "l_cbow = L_CBOW_classifier(len(word2idx), 50)\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "dev_accuracies = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(model, max_epochs=20):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr =lr, weight_decay=1e-4)\n",
    "#     model.train()\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "        \n",
    "        n=0\n",
    "        for x, labels in train_loader:\n",
    "            n+=batch_size\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            train_mean_loss+=loss.item()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    train_accuracy+=1\n",
    "                \n",
    "            if(n%250==0):\n",
    "                print(\"step : {}/{} \".format(n, len(train_data)))\n",
    "                print(\"step accuracy : \", train_accuracy/n)\n",
    "                print(\"step loss : \", train_mean_loss/n)\n",
    "                \n",
    "        ###Dev test\n",
    "        for x, labels in dev_loader:\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            dev_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    dev_accuracy+=1\n",
    "                    \n",
    "        train_accuracies.append(train_accuracy/len(train_data))\n",
    "        train_losses.append(train_mean_loss/len(train_data))\n",
    "        dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        \n",
    "        \n",
    "        print(\"EPOCH {}\".format(e+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 1500/3206 \n",
      "step accuracy :  0.6273333333333333\n",
      "step loss :  0.029109583854675294\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.6413333333333333\n",
      "step loss :  0.02749831024805705\n",
      "EPOCH 1\n",
      "Train Accuracy :  0.6378665003119152\n",
      "Dev Accuracy :  0.628428927680798\n",
      "Train Mean loss :  0.02839727291671172\n",
      "Dev Mean loss :  0.03711528017039311\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.712\n",
      "step loss :  0.018421828746795654\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.7523333333333333\n",
      "step loss :  0.014224151611328124\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.7551466001247661\n",
      "Dev Accuracy :  0.8678304239401496\n",
      "Train Mean loss :  0.013802091255830514\n",
      "Dev Mean loss :  0.008222416749321611\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.8826666666666667\n",
      "step loss :  0.005967991272608439\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.8633333333333333\n",
      "step loss :  0.005540942251682282\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.8686837180286962\n",
      "Dev Accuracy :  0.8478802992518704\n",
      "Train Mean loss :  0.005492072419977456\n",
      "Dev Mean loss :  0.005812156081496926\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9166666666666666\n",
      "step loss :  0.002955932060877482\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.8826666666666667\n",
      "step loss :  0.0038270527323087055\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.8867747972551466\n",
      "Dev Accuracy :  0.9276807980049875\n",
      "Train Mean loss :  0.00391210774593032\n",
      "Dev Mean loss :  0.004171269790192792\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9513333333333334\n",
      "step loss :  0.0020970651308695474\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9336666666666666\n",
      "step loss :  0.0021509203016757965\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.932626325639426\n",
      "Dev Accuracy :  0.8977556109725686\n",
      "Train Mean loss :  0.002229226849559539\n",
      "Dev Mean loss :  0.0053714686201100335\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9486666666666667\n",
      "step loss :  0.0016422041058540345\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9546666666666667\n",
      "step loss :  0.0016397810578346252\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.9541484716157205\n",
      "Dev Accuracy :  0.9276807980049875\n",
      "Train Mean loss :  0.001761229668121671\n",
      "Dev Mean loss :  0.004086500540041269\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9593333333333334\n",
      "step loss :  0.0014923821886380514\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9433333333333334\n",
      "step loss :  0.0016285432875156403\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.932626325639426\n",
      "Dev Accuracy :  0.8778054862842892\n",
      "Train Mean loss :  0.0019852809343052446\n",
      "Dev Mean loss :  0.004689052812476408\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.948\n",
      "step loss :  0.001400615264972051\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.92\n",
      "step loss :  0.0024445832073688506\n",
      "EPOCH 8\n",
      "Train Accuracy :  0.924516531503431\n",
      "Dev Accuracy :  0.9226932668329177\n",
      "Train Mean loss :  0.0022945683710951024\n",
      "Dev Mean loss :  0.003626588351114135\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9266666666666666\n",
      "step loss :  0.002068903883298238\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.935\n",
      "step loss :  0.001659742680688699\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.9376169681846538\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.001718023335253233\n",
      "Dev Mean loss :  0.0023917833022643205\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.982\n",
      "step loss :  0.0009016110996405283\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9563333333333334\n",
      "step loss :  0.0013465254108111064\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.9585152838427947\n",
      "Dev Accuracy :  0.9376558603491272\n",
      "Train Mean loss :  0.0012642944975874049\n",
      "Dev Mean loss :  0.0037446219724907243\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9766666666666667\n",
      "step loss :  0.0009306143522262573\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9776666666666667\n",
      "step loss :  0.0009574361667037011\n",
      "EPOCH 11\n",
      "Train Accuracy :  0.9778540237055521\n",
      "Dev Accuracy :  0.9501246882793017\n",
      "Train Mean loss :  0.0009745109758931848\n",
      "Dev Mean loss :  0.0025806420313152587\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.974\n",
      "step loss :  0.0007922865798075994\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.972\n",
      "step loss :  0.0010987563629945118\n",
      "EPOCH 12\n",
      "Train Accuracy :  0.9731752963194011\n",
      "Dev Accuracy :  0.9551122194513716\n",
      "Train Mean loss :  0.0010379627672345358\n",
      "Dev Mean loss :  0.002790414112761728\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9846666666666667\n",
      "step loss :  0.0008759250740210216\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9856666666666667\n",
      "step loss :  0.0007484469413757324\n",
      "EPOCH 13\n",
      "Train Accuracy :  0.9853399875233937\n",
      "Dev Accuracy :  0.940149625935162\n",
      "Train Mean loss :  0.0007531964920196247\n",
      "Dev Mean loss :  0.003518069102579816\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9826666666666667\n",
      "step loss :  0.0007521804794669151\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.982\n",
      "step loss :  0.0007244915527602037\n",
      "EPOCH 14\n",
      "Train Accuracy :  0.9831565814098565\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.0006793648882113114\n",
      "Dev Mean loss :  0.003849005312693684\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9726666666666667\n",
      "step loss :  0.0007734346116582552\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9706666666666667\n",
      "step loss :  0.0008127205185592174\n",
      "EPOCH 15\n",
      "Train Accuracy :  0.9706799750467873\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.000832173093707206\n",
      "Dev Mean loss :  0.0031955250332183077\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9826666666666667\n",
      "step loss :  0.0006924541583284736\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.976\n",
      "step loss :  0.0008558536192091803\n",
      "EPOCH 16\n",
      "Train Accuracy :  0.9772301933873986\n",
      "Dev Accuracy :  0.9551122194513716\n",
      "Train Mean loss :  0.0008429922670397863\n",
      "Dev Mean loss :  0.002657168375286378\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9873333333333333\n",
      "step loss :  0.0006399836887915929\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9863333333333333\n",
      "step loss :  0.0006850693759818872\n",
      "EPOCH 17\n",
      "Train Accuracy :  0.9831565814098565\n",
      "Dev Accuracy :  0.940149625935162\n",
      "Train Mean loss :  0.0007245775372607962\n",
      "Dev Mean loss :  0.004097352004110664\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.978\n",
      "step loss :  0.0011239762902259826\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9863333333333333\n",
      "step loss :  0.0007170525263063609\n",
      "EPOCH 18\n",
      "Train Accuracy :  0.9865876481597006\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.0007153411303761317\n",
      "Dev Mean loss :  0.0035477762953598896\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.986\n",
      "step loss :  0.0007230695138374964\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.986\n",
      "step loss :  0.0006270751282572746\n",
      "EPOCH 19\n",
      "Train Accuracy :  0.9859638178415471\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.0005949960222104082\n",
      "Dev Mean loss :  0.002003271346377613\n",
      "----------------------------------------\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9946666666666667\n",
      "step loss :  0.0003158861255894105\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9776666666666667\n",
      "step loss :  0.0007239461047574878\n",
      "EPOCH 20\n",
      "Train Accuracy :  0.96756082345602\n",
      "Dev Accuracy :  0.9551122194513716\n",
      "Train Mean loss :  0.0009531817045725106\n",
      "Dev Mean loss :  0.0024699982710907286\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_cbow(l_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970074812967581\n"
     ]
    }
   ],
   "source": [
    "print(max(dev_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbow(model, test_data, test_labels):\n",
    "    acc = 0\n",
    "    test_mean_loss = 0\n",
    "    \n",
    "    for x, labels in test_loader:\n",
    "            \n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            test_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    acc+=1\n",
    "    print(\"Test accuracy : \", acc/len(test_data))\n",
    "    print(\"Test mean loss : \", test_mean_loss/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.9427860696517413\n",
      "Test mean loss :  0.002640826340338484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothée\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2016: UserWarning: Using a target size (torch.Size([102])) that is different to the input size (torch.Size([102, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "test_cbow(l_cbow, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un classifieur RNN classique (GRU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_classifier(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, vocab_size, embedding_dim, rnn_dropout, bidirectional=False):\n",
    "        super(GRU_classifier, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout,bidirectional=bidirectional)\n",
    "\n",
    "        if bidirectional:    \n",
    "            self.fc = nn.Linear(2*hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.is_bidirectional = bidirectional \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "            \n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(0)\n",
    "        embeds = self.embedding(inputs)\n",
    "        rnn_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        out = self.dropout(rnn_out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = th.sigmoid(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_bidirectional:\n",
    "            hidden = weight.new(2*self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La même classe mais avec de l'attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_with_attention_classifier(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, vocab_size, embedding_dim, rnn_dropout, bidirectional=False):\n",
    "        super(GRU_with_attention_classifier, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout,bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:    \n",
    "            self.fc = nn.Linear(2*hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "            \n",
    "        self.is_bidirectional = bidirectional \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "        self.fc_attn1 = nn.Linear(hidden_size, 100)\n",
    "        self.fc_attn2 = nn.Linear(100,1, bias=False)    \n",
    "        self.fc_attn3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(0)\n",
    "        embeds = self.embedding(inputs)\n",
    "        rnn_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        out = self.dropout(rnn_out)\n",
    "        attention_weight = self.fc_attn1(out)\n",
    "        attention_weight = self.tanh(attention_weight)\n",
    "        attention_weight = self.softmax(self.fc_attn2(attention_weight))\n",
    "        out = (out*attention_weight).sum(dim=1)\n",
    "        out = self.fc_attn3(out)\n",
    "        out = th.sigmoid(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_bidirectional:\n",
    "            hidden = weight.new(2*self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_with_attention_classifier(\n",
      "  (embedding): Embedding(43464, 10)\n",
      "  (gru): GRU(10, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc_attn1): Linear(in_features=32, out_features=100, bias=True)\n",
      "  (fc_attn2): Linear(in_features=100, out_features=1, bias=False)\n",
      "  (fc_attn3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##Hyper-paramètres\n",
    "nb_cells = 1\n",
    "hidden_size = 32\n",
    "embedding_dim = 10\n",
    "learning_rate = 1e-2\n",
    "loss_fn = nn.BCELoss()\n",
    "m = GRU_with_attention_classifier(nb_cells, hidden_size, vocab_size, embedding_dim, 0.0, bidirectional=False)\n",
    "#m = GRU_classifier(nb_cells, hidden_size, vocab_size, embedding_dim, 0.0, bidirectional=False)\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, batch_size,lr, max_epochs=10, conv=False):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    best_dict = model.state_dict()\n",
    "    best_acc = 0\n",
    "    for i in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "\n",
    "        n = 0 \n",
    "\n",
    "        for x, labels in train_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            h = h.data\n",
    "            \n",
    "            n+=loc_batch_size\n",
    "            \n",
    "            preds, _ = model(x, h)  \n",
    "            loss = loss_fn(preds, labels)\n",
    "            h = h.detach()\n",
    "            train_mean_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                    train_accuracy+=1\n",
    "        \n",
    "            if(n%250==0):\n",
    "                print(\"step : {}/{} \".format(n, len(train_data)))\n",
    "                print(\"step accuracy : \", train_accuracy/n)\n",
    "                print(\"step loss : \", train_mean_loss/n)\n",
    "                \n",
    "        for x, labels in dev_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            preds, _ = model(x, h)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            dev_mean_loss+=loss.item()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                    dev_accuracy+=1\n",
    "        \n",
    "        if(i==0):\n",
    "            best_dict=model.state_dict()\n",
    "            best_acc=dev_accuracy/len(dev_data)\n",
    "        else:\n",
    "            if(best_acc<(dev_accuracy/len(dev_data))):\n",
    "                best_acc=dev_accuracy/len(dev_data)\n",
    "                best_dict=model.state_dict()\n",
    "                print(\"new best acc\")\n",
    "        \n",
    "        print(\"EPOCH {}\".format(i+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")\n",
    "        \n",
    "#     model.load_state_dict(best_dict)\n",
    "#     acc = test(model, test_data, conv=conv)\n",
    "#     stat_dict[model_name][4].append(acc)\n",
    "#     print(\"Accuracy on test data : \", acc)\n",
    "#     return best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 250/3206 \n",
      "step accuracy :  0.576\n",
      "step loss :  0.013595621824264527\n",
      "step : 500/3206 \n",
      "step accuracy :  0.572\n",
      "step loss :  0.013630937457084656\n",
      "step : 750/3206 \n",
      "step accuracy :  0.5813333333333334\n",
      "step loss :  0.013452350695927939\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.591\n",
      "step loss :  0.013351022362709046\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.6064\n",
      "step loss :  0.013296894598007203\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.6166666666666667\n",
      "step loss :  0.013116304198900858\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.6165714285714285\n",
      "step loss :  0.01305451910836356\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.6205\n",
      "step loss :  0.012949522465467453\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.6248888888888889\n",
      "step loss :  0.01285066941049364\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.6344\n",
      "step loss :  0.01267199878692627\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.6389090909090909\n",
      "step loss :  0.012607661659067328\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.649\n",
      "step loss :  0.01247554690639178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothée\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.6512788521522146\n",
      "Dev Accuracy :  0.7955112219451371\n",
      "Train Mean loss :  0.012593304476958101\n",
      "Dev Mean loss :  0.01032596856281347\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.78\n",
      "step loss :  0.010204473853111268\n",
      "step : 500/3206 \n",
      "step accuracy :  0.806\n",
      "step loss :  0.00952898108959198\n",
      "step : 750/3206 \n",
      "step accuracy :  0.8213333333333334\n",
      "step loss :  0.009120239893595377\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.818\n",
      "step loss :  0.009129592806100845\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.808\n",
      "step loss :  0.009299426054954528\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.806\n",
      "step loss :  0.009323635756969453\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.8017142857142857\n",
      "step loss :  0.009308373042515345\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.803\n",
      "step loss :  0.00916855876147747\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.8044444444444444\n",
      "step loss :  0.008987234036127726\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.808\n",
      "step loss :  0.008861706125736236\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.8087272727272727\n",
      "step loss :  0.008808976877819409\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.815\n",
      "step loss :  0.008610483646392822\n",
      "new best acc\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.8162819713038054\n",
      "Dev Accuracy :  0.8753117206982544\n",
      "Train Mean loss :  0.00862121092142794\n",
      "Dev Mean loss :  0.005661928464201026\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.956\n",
      "step loss :  0.004006401091814041\n",
      "step : 500/3206 \n",
      "step accuracy :  0.936\n",
      "step loss :  0.004935063824057579\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9266666666666666\n",
      "step loss :  0.004744834631681442\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.932\n",
      "step loss :  0.00437606567889452\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9376\n",
      "step loss :  0.00400963329076767\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9393333333333334\n",
      "step loss :  0.003907633423805237\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9428571428571428\n",
      "step loss :  0.0036775021978787013\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.943\n",
      "step loss :  0.0035494452808052302\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9431111111111111\n",
      "step loss :  0.003495114983783828\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.944\n",
      "step loss :  0.003372609579563141\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9454545454545454\n",
      "step loss :  0.003295702777125619\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.948\n",
      "step loss :  0.003182009334365527\n",
      "new best acc\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.950093574547723\n",
      "Dev Accuracy :  0.9625935162094763\n",
      "Train Mean loss :  0.003117799509086612\n",
      "Dev Mean loss :  0.0031442524321804617\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.984\n",
      "step loss :  0.0011625721380114555\n",
      "step : 500/3206 \n",
      "step accuracy :  0.978\n",
      "step loss :  0.0016450861040502786\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9773333333333334\n",
      "step loss :  0.001661184836179018\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.979\n",
      "step loss :  0.0015196616682223976\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9824\n",
      "step loss :  0.0013469228636473418\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9833333333333333\n",
      "step loss :  0.0012183180560047429\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.984\n",
      "step loss :  0.0012474806167717492\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.982\n",
      "step loss :  0.0013768598686438053\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9831111111111112\n",
      "step loss :  0.0013288505787236824\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9836\n",
      "step loss :  0.0013071782020851969\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.982909090909091\n",
      "step loss :  0.0013177715240215713\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9816666666666667\n",
      "step loss :  0.0013558930604097743\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.9812850904553961\n",
      "Dev Accuracy :  0.9576059850374065\n",
      "Train Mean loss :  0.0015050021410673968\n",
      "Dev Mean loss :  0.0025193633655657493\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0005099812373518944\n",
      "step : 500/3206 \n",
      "step accuracy :  0.99\n",
      "step loss :  0.0006113882604986429\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9906666666666667\n",
      "step loss :  0.0006294225305318832\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0005560576650314033\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9912\n",
      "step loss :  0.0006050980668514967\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0005720326680069168\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9925714285714285\n",
      "step loss :  0.0005331320914306811\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.993\n",
      "step loss :  0.0005486149101052433\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9928888888888889\n",
      "step loss :  0.0006048539605819517\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9932\n",
      "step loss :  0.0005648638660088181\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9934545454545455\n",
      "step loss :  0.0005560561890967867\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.993\n",
      "step loss :  0.0005744320447556674\n",
      "new best acc\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.991890205864005\n",
      "Dev Accuracy :  0.9650872817955112\n",
      "Train Mean loss :  0.0006560229566192262\n",
      "Dev Mean loss :  0.0021018062534154465\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0006257492154836655\n",
      "step : 500/3206 \n",
      "step accuracy :  0.994\n",
      "step loss :  0.0005376049489714206\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9946666666666667\n",
      "step loss :  0.00043849778920412066\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.994\n",
      "step loss :  0.0005089592272415757\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9952\n",
      "step loss :  0.0004345658991485834\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.996\n",
      "step loss :  0.0003827379702900847\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9948571428571429\n",
      "step loss :  0.00047184429091534446\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.9935\n",
      "step loss :  0.0004867224855115637\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9937777777777778\n",
      "step loss :  0.0004541524660049213\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.994\n",
      "step loss :  0.00047334011970087887\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9945454545454545\n",
      "step loss :  0.00044928648686883124\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.995\n",
      "step loss :  0.0004312704021576792\n",
      "new best acc\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.9953212726138491\n",
      "Dev Accuracy :  0.9675810473815462\n",
      "Train Mean loss :  0.00040955453565261717\n",
      "Dev Mean loss :  0.0026103083068676793\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  4.917720938101411e-05\n",
      "step : 500/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  4.745119088329375e-05\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9986666666666667\n",
      "step loss :  0.00011694211109230916\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.998\n",
      "step loss :  0.00017249742755666376\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9976\n",
      "step loss :  0.00018725120909512044\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9953333333333333\n",
      "step loss :  0.00030970344475160044\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.996\n",
      "step loss :  0.00031396618166140147\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.9945\n",
      "step loss :  0.00048482351331040265\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0006612112636988362\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9884\n",
      "step loss :  0.000905896724294871\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9883636363636363\n",
      "step loss :  0.0008915470041842623\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.9876666666666667\n",
      "step loss :  0.0009164585970186939\n",
      "new best acc\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.9881472239550843\n",
      "Dev Accuracy :  0.972568578553616\n",
      "Train Mean loss :  0.000884298083467696\n",
      "Dev Mean loss :  0.002226667374168102\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0007740997299551964\n",
      "step : 500/3206 \n",
      "step accuracy :  0.99\n",
      "step loss :  0.0008964305184781551\n",
      "step : 750/3206 \n",
      "step accuracy :  0.988\n",
      "step loss :  0.0009066032245755196\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.986\n",
      "step loss :  0.0009670760645531118\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9856\n",
      "step loss :  0.0010117554616183042\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9866666666666667\n",
      "step loss :  0.0009181363818546136\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9868571428571429\n",
      "step loss :  0.0008641299331294638\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.988\n",
      "step loss :  0.0008281931490637363\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9893333333333333\n",
      "step loss :  0.0007593796035895745\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9896\n",
      "step loss :  0.0007411042079329491\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9905454545454545\n",
      "step loss :  0.0006886133358559825\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.991\n",
      "step loss :  0.0006757955845290174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8\n",
      "Train Accuracy :  0.9912663755458515\n",
      "Dev Accuracy :  0.9650872817955112\n",
      "Train Mean loss :  0.0006448506819078767\n",
      "Dev Mean loss :  0.0045491936134680775\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  0.988\n",
      "step loss :  0.001235723470337689\n",
      "step : 500/3206 \n",
      "step accuracy :  0.984\n",
      "step loss :  0.0013283347520045936\n",
      "step : 750/3206 \n",
      "step accuracy :  0.984\n",
      "step loss :  0.0011115919385726253\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.986\n",
      "step loss :  0.000930727364262566\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9872\n",
      "step loss :  0.0008918981378898025\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9886666666666667\n",
      "step loss :  0.0007858748990111054\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9885714285714285\n",
      "step loss :  0.0008095772614968675\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.9895\n",
      "step loss :  0.0007533612898550928\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9897777777777778\n",
      "step loss :  0.0007497105799201462\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9904\n",
      "step loss :  0.0007440367992967368\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9912727272727273\n",
      "step loss :  0.0006928707750683481\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.992\n",
      "step loss :  0.0006453813200350851\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.9922021210230817\n",
      "Dev Accuracy :  0.970074812967581\n",
      "Train Mean loss :  0.0006489223628191606\n",
      "Dev Mean loss :  0.0026582084628672077\n",
      "----------------------------------------\n",
      "step : 250/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  8.435712102800608e-05\n",
      "step : 500/3206 \n",
      "step accuracy :  0.998\n",
      "step loss :  0.00021813492104411124\n",
      "step : 750/3206 \n",
      "step accuracy :  0.9973333333333333\n",
      "step loss :  0.00021505265946810445\n",
      "step : 1000/3206 \n",
      "step accuracy :  0.998\n",
      "step loss :  0.00017884639475960286\n",
      "step : 1250/3206 \n",
      "step accuracy :  0.9968\n",
      "step loss :  0.00027401182139292357\n",
      "step : 1500/3206 \n",
      "step accuracy :  0.9966666666666667\n",
      "step loss :  0.00029724571225233374\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9965714285714286\n",
      "step loss :  0.00031051844112308963\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.996\n",
      "step loss :  0.0003471444240421988\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9964444444444445\n",
      "step loss :  0.0003254971701341371\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9964\n",
      "step loss :  0.0003196545431856066\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9963636363636363\n",
      "step loss :  0.00030411573800004344\n",
      "step : 3000/3206 \n",
      "step accuracy :  0.996\n",
      "step loss :  0.00030078843181642395\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.9962570180910792\n",
      "Dev Accuracy :  0.9675810473815462\n",
      "Train Mean loss :  0.00028374174374698344\n",
      "Dev Mean loss :  0.003790187347794133\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_rnn(m, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn(model, test_data, test_labels):\n",
    "    acc = 0\n",
    "    test_mean_loss = 0\n",
    "    \n",
    "    for x, labels in test_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            preds, _ = model(x, h)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            test_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    acc+=1\n",
    "    print(\"Test accuracy : \", acc/len(test_data))\n",
    "    print(\"Test mean loss : \", test_mean_loss/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.9651741293532339\n",
      "Test mean loss :  0.003409405364029443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothée\\Anaconda3\\envs\\dev\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_rnn(m, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
