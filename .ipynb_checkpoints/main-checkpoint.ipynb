{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des données et construction des train, dev et test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##On transforme les 3 colonnes en 1 : url, titre et le texte deviennent un seul élément\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, encoding=\"utf8\", newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            txt = clean_str(row[2])\n",
    "            txt = \" \".join(txt.split(\"\\n\"))\n",
    "#             d = clean_str(row[0])+\" \"+clean_str(row[1])\n",
    "            labels.append(row[3])\n",
    "            data.append(txt)    \n",
    "    data.pop(0)\n",
    "    labels.pop(0)\n",
    "    labels = [float(l) for l in labels]\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_data(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image copyright getty images on sunday morning , donald trump went off on a twitter tirade against a member of his own party this , in itself , is n't exactly huge news it 's far from the first time the president has turned his rhetorical cannons on his own ranks this time , however , his attacks were particularly biting and personal he essentially called tennessee senator bob corker , the chair of the powerful senate foreign relations committee , a coward for not running for re election he said mr corker begged for the president 's endorsement , which he refused to give he wrongly claimed that mr corker 's support of the iranian nuclear agreement was his only political accomplishment unlike some of his colleagues , mr corker free from having to worry about his immediate political future did n't hold his tongue skip twitter post by senbobcorker it 's a shame the white house has become an adult day care center someone obviously missed their shift this morning senator bob corker \\( senbobcorker \\) october 8 , 2017 report that was n't the end of it , though he then spoke with the new york times and really let the president have it here are four choice quotes from the tennessee senator 's interview with the times and why they are particularly damning i do n't know why the president tweets out things that are not true you know he does it , everyone knows he does it , but he does you ca n't really sugarcoat this one mr corker is flat out saying the president is a liar and everyone knows it the senator , in particular , is challenging mr trump 's insistence that he unsuccessfully pleaded for his endorsement , but the accusation is much broader mr corker and the president used to be something akin to allies the tennessean was on mr trump 's short list for vice president and secretary of state image copyright getty images image caption bob corker at trump campaign rally in july 2016 those days are seemingly very much over now and it 's not like mr corker is going anywhere anytime soon although he 's not running for re election , he 'll be in the senate , chairing a powerful committee , until january 2019 the president 's margin for success in that chamber is razor thin if democrats can continue to stand together in opposition , he can afford to lose only two votes out of 52 republican senators that 's why healthcare reform collapsed in july and it could be bad news for tax efforts from here on out , mr corker is n't going to do the president any favours look , except for a few people , the vast majority of our caucus understands what we 're dealing with here frustration in congress has been growing over what republicans feel has been the president 's inability to focus on advancing their agenda getting a sharply divided party to come together on plans to repeal obamacare , reform taxes or boost infrastructure spending is challenging enough doing so when the president stirs up unrelated controversies on a seemingly daily basis makes things all the harder one of the president 's gifts has been his ability to shake off negative stories by quickly moving on to a different subject that worked brilliantly during his presidential campaign , but it 's less effective during the legislative slow grind image copyright getty images image caption corker at the confirmation hearing for secretary of state rex tillerson for months , republicans in congress have been grumbling about this in the background and among themselves occasionally , someone like mr mcconnell will lament that the president does n't understand how the senate works mr corker has now stated it loud and clear and , what 's more , he says almost everyone agrees with him they 've kept silent until now because they still hope to pass conservative legislation that the president can sign or fear mr trump 's legions will back a primary challenge next year or stay home during the general election if that calculus ever changes if it becomes riskier to stay silent than speak out mr trump will be in real trouble a lot of people think that there is some kind of 'good cop , bad cop' act underway , but that 's just not true time and again , mr trump has appeared to undercut secretary of state rex tillerson and others in his administration who are attempting to use soft diplomacy to deal with a range of international crises the war against the taliban in afghanistan , iran 's compliance with the multinational nuclear agreement , the ongoing dispute between qatar and its persian gulf neighbours , the unrest in venezuela and , most recently , north korea 's continued ballistic missile tests have all been the target of the president 's offhand remarks and twitter invective some administration defenders have said this is all a part of mr trump 's strategy an updated version of the nixon era madman theory , in which the president forces adversaries to give way because they fear an unpredictable us leader 's actions mr corker is n't buying it there 's no strategy , he says , just the possibility of chaos which he hopes mr trump 's senior advisers will be able to avoid i know for a fact that every single day at the white house , it 's a situation of trying to contain him there 's now a growing collection of john kelly face palm photos that serve as a testament to the chief of staff 's reported frustration at dealing with the president mr trump goes off script to praise torch bearing white nationalists at a rally in charlottesville , and mr kelly is captured closing his eyes and rubbing the arch of his nose , as if attempting to stave off a migraine image copyright reuters image caption white house chief of staff john kelly looks on as us president donald trump speaks at a campaign rally the president calls north korean leaders criminals in a speech to the united nations , and mr kelly straight up buries his face in his hands the white house communications team is often left scrambling to try to explain or reframe an indelicate presidential joke or remark that directly contradicts what was until then the official administration line even though mr kelly has brought some discipline to the west wing staff , the president still marches to the beat of his own drum and continues to have unfettered access to his phone 's twitter app bob corker is only the latest person politician , journalist , sports star or celebrity to feel the mercurial president 's uncontainable ire\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4009,)\n",
      "Nombre de fake news : 2137\n",
      "Nombre de news correctes : 1872\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "fake_data = data[labels==0]\n",
    "real_data = data[labels==1]\n",
    "fake_labels = labels[labels==0]\n",
    "real_labels = labels[labels==1]\n",
    "print(\"Nombre de fake news : {}\".format(len(fake_data)))\n",
    "print(\"Nombre de news correctes : {}\".format(len(real_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2137,)\n"
     ]
    }
   ],
   "source": [
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([*real_data[:int(len(real_data)*0.8)],*fake_data[:int(len(fake_data)*0.8)]])\n",
    "train_labels = np.array([*real_labels[:int(len(real_labels)*0.8)],*fake_labels[:int(len(fake_labels)*0.8)]], dtype=np.float32)\n",
    "\n",
    "dev_data = np.array([*real_data[int(len(real_data)*0.8):int(len(real_data)*0.9)], *fake_data[int(len(fake_data)*0.8):int(len(fake_data)*0.9)]])\n",
    "dev_labels = np.array([*real_labels[int(len(real_labels)*0.8):int(len(real_labels)*0.9)], *fake_labels[int(len(fake_labels)*0.8):int(len(fake_labels)*0.9)]], dtype=np.float32)\n",
    "\n",
    "test_data = np.array([*real_data[int(len(real_data)*0.9):], *fake_data[int(len(fake_data)*0.9):]])\n",
    "test_labels = np.array([*real_labels[int(len(real_labels)*0.9):], *fake_labels[int(len(fake_labels)*0.9):]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du train set : 3206\n",
      "taille du dev set : 401\n",
      "taille du test set : 402\n"
     ]
    }
   ],
   "source": [
    "print(\"taille du train set : {}\".format(len(train_data)))\n",
    "print(\"taille du dev set : {}\".format(len(dev_data)))\n",
    "print(\"taille du test set : {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'm', 'a', 'g', 'e', ' ', 'c', 'o', 'p', 'y', 'r', 'i', 'g', 'h', 't', ' ', 'g', 'e', 't', 't', 'y', ' ', 'i', 'm', 'a', 'g', 'e', 's', ' ', 'o', 'n', ' ', 's', 'u', 'n', 'd', 'a', 'y', ' ', 'm', 'o', 'r', 'n', 'i', 'n', 'g', ' ', ',', ' ', 'd', 'o', 'n', 'a', 'l', 'd', ' ', 't', 'r', 'u', 'm', 'p', ' ', 'w', 'e', 'n', 't', ' ', 'o', 'f', 'f', ' ', 'o', 'n', ' ', 'a', ' ', 't', 'w', 'i', 't', 't', 'e', 'r', ' ', 't', 'i', 'r', 'a', 'd', 'e', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 'a', ' ', 'm', 'e', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'p', 'a', 'r', 't', 'y', ' ', 't', 'h', 'i', 's', ' ', ',', ' ', 'i', 'n', ' ', 'i', 't', 's', 'e', 'l', 'f', ' ', ',', ' ', 'i', 's', ' ', 'n', \"'\", 't', ' ', 'e', 'x', 'a', 'c', 't', 'l', 'y', ' ', 'h', 'u', 'g', 'e', ' ', 'n', 'e', 'w', 's', ' ', 'i', 't', ' ', \"'\", 's', ' ', 'f', 'a', 'r', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'i', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'h', 'a', 's', ' ', 't', 'u', 'r', 'n', 'e', 'd', ' ', 'h', 'i', 's', ' ', 'r', 'h', 'e', 't', 'o', 'r', 'i', 'c', 'a', 'l', ' ', 'c', 'a', 'n', 'n', 'o', 'n', 's', ' ', 'o', 'n', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'r', 'a', 'n', 'k', 's', ' ', 't', 'h', 'i', 's', ' ', 't', 'i', 'm', 'e', ' ', ',', ' ', 'h', 'o', 'w', 'e', 'v', 'e', 'r', ' ', ',', ' ', 'h', 'i', 's', ' ', 'a', 't', 't', 'a', 'c', 'k', 's', ' ', 'w', 'e', 'r', 'e', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 'l', 'y', ' ', 'b', 'i', 't', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 'p', 'e', 'r', 's', 'o', 'n', 'a', 'l', ' ', 'h', 'e', ' ', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', 'l', 'y', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 't', 'e', 'n', 'n', 'e', 's', 's', 'e', 'e', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', ' ', 'b', 'o', 'b', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', ',', ' ', 't', 'h', 'e', ' ', 'c', 'h', 'a', 'i', 'r', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 's', 'e', 'n', 'a', 't', 'e', ' ', 'f', 'o', 'r', 'e', 'i', 'g', 'n', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'o', 'n', 's', ' ', 'c', 'o', 'm', 'm', 'i', 't', 't', 'e', 'e', ' ', ',', ' ', 'a', ' ', 'c', 'o', 'w', 'a', 'r', 'd', ' ', 'f', 'o', 'r', ' ', 'n', 'o', 't', ' ', 'r', 'u', 'n', 'n', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 'r', 'e', ' ', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'h', 'e', ' ', 's', 'a', 'i', 'd', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'b', 'e', 'g', 'g', 'e', 'd', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'e', 'n', 'd', 'o', 'r', 's', 'e', 'm', 'e', 'n', 't', ' ', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'h', 'e', ' ', 'r', 'e', 'f', 'u', 's', 'e', 'd', ' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 'h', 'e', ' ', 'w', 'r', 'o', 'n', 'g', 'l', 'y', ' ', 'c', 'l', 'a', 'i', 'm', 'e', 'd', ' ', 't', 'h', 'a', 't', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', \"'\", 's', ' ', 's', 'u', 'p', 'p', 'o', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'r', 'a', 'n', 'i', 'a', 'n', ' ', 'n', 'u', 'c', 'l', 'e', 'a', 'r', ' ', 'a', 'g', 'r', 'e', 'e', 'm', 'e', 'n', 't', ' ', 'w', 'a', 's', ' ', 'h', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', 'p', 'o', 'l', 'i', 't', 'i', 'c', 'a', 'l', ' ', 'a', 'c', 'c', 'o', 'm', 'p', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', ' ', 'u', 'n', 'l', 'i', 'k', 'e', ' ', 's', 'o', 'm', 'e', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'c', 'o', 'l', 'l', 'e', 'a', 'g', 'u', 'e', 's', ' ', ',', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'f', 'r', 'e', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 's', ' ', 'i', 'm', 'm', 'e', 'd', 'i', 'a', 't', 'e', ' ', 'p', 'o', 'l', 'i', 't', 'i', 'c', 'a', 'l', ' ', 'f', 'u', 't', 'u', 'r', 'e', ' ', 'd', 'i', 'd', ' ', 'n', \"'\", 't', ' ', 'h', 'o', 'l', 'd', ' ', 'h', 'i', 's', ' ', 't', 'o', 'n', 'g', 'u', 'e', ' ', 's', 'k', 'i', 'p', ' ', 't', 'w', 'i', 't', 't', 'e', 'r', ' ', 'p', 'o', 's', 't', ' ', 'b', 'y', ' ', 's', 'e', 'n', 'b', 'o', 'b', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 't', ' ', \"'\", 's', ' ', 'a', ' ', 's', 'h', 'a', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'w', 'h', 'i', 't', 'e', ' ', 'h', 'o', 'u', 's', 'e', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'c', 'o', 'm', 'e', ' ', 'a', 'n', ' ', 'a', 'd', 'u', 'l', 't', ' ', 'd', 'a', 'y', ' ', 'c', 'a', 'r', 'e', ' ', 'c', 'e', 'n', 't', 'e', 'r', ' ', 's', 'o', 'm', 'e', 'o', 'n', 'e', ' ', 'o', 'b', 'v', 'i', 'o', 'u', 's', 'l', 'y', ' ', 'm', 'i', 's', 's', 'e', 'd', ' ', 't', 'h', 'e', 'i', 'r', ' ', 's', 'h', 'i', 'f', 't', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'r', 'n', 'i', 'n', 'g', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', ' ', 'b', 'o', 'b', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', '\\\\', '(', ' ', 's', 'e', 'n', 'b', 'o', 'b', 'c', 'o', 'r', 'k', 'e', 'r', ' ', '\\\\', ')', ' ', 'o', 'c', 't', 'o', 'b', 'e', 'r', ' ', '8', ' ', ',', ' ', '2', '0', '1', '7', ' ', 'r', 'e', 'p', 'o', 'r', 't', ' ', 't', 'h', 'a', 't', ' ', 'w', 'a', 's', ' ', 'n', \"'\", 't', ' ', 't', 'h', 'e', ' ', 'e', 'n', 'd', ' ', 'o', 'f', ' ', 'i', 't', ' ', ',', ' ', 't', 'h', 'o', 'u', 'g', 'h', ' ', 'h', 'e', ' ', 't', 'h', 'e', 'n', ' ', 's', 'p', 'o', 'k', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'w', ' ', 'y', 'o', 'r', 'k', ' ', 't', 'i', 'm', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'l', 'e', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'h', 'a', 'v', 'e', ' ', 'i', 't', ' ', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'f', 'o', 'u', 'r', ' ', 'c', 'h', 'o', 'i', 'c', 'e', ' ', 'q', 'u', 'o', 't', 'e', 's', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 't', 'e', 'n', 'n', 'e', 's', 's', 'e', 'e', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', ' ', \"'\", 's', ' ', 'i', 'n', 't', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 't', 'i', 'm', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'w', 'h', 'y', ' ', 't', 'h', 'e', 'y', ' ', 'a', 'r', 'e', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 'l', 'y', ' ', 'd', 'a', 'm', 'n', 'i', 'n', 'g', ' ', 'i', ' ', 'd', 'o', ' ', 'n', \"'\", 't', ' ', 'k', 'n', 'o', 'w', ' ', 'w', 'h', 'y', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 't', 'w', 'e', 'e', 't', 's', ' ', 'o', 'u', 't', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 't', 'h', 'a', 't', ' ', 'a', 'r', 'e', ' ', 'n', 'o', 't', ' ', 't', 'r', 'u', 'e', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'h', 'e', ' ', 'd', 'o', 'e', 's', ' ', 'i', 't', ' ', ',', ' ', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'k', 'n', 'o', 'w', 's', ' ', 'h', 'e', ' ', 'd', 'o', 'e', 's', ' ', 'i', 't', ' ', ',', ' ', 'b', 'u', 't', ' ', 'h', 'e', ' ', 'd', 'o', 'e', 's', ' ', 'y', 'o', 'u', ' ', 'c', 'a', ' ', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 's', 'u', 'g', 'a', 'r', 'c', 'o', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'o', 'n', 'e', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'f', 'l', 'a', 't', ' ', 'o', 'u', 't', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'i', 'a', 'r', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'k', 'n', 'o', 'w', 's', ' ', 'i', 't', ' ', 't', 'h', 'e', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', ' ', ',', ' ', 'i', 'n', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', ' ', ',', ' ', 'i', 's', ' ', 'c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'i', 'n', 'g', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', \"'\", 's', ' ', 'i', 'n', 's', 'i', 's', 't', 'e', 'n', 'c', 'e', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'u', 'n', 's', 'u', 'c', 'c', 'e', 's', 's', 'f', 'u', 'l', 'l', 'y', ' ', 'p', 'l', 'e', 'a', 'd', 'e', 'd', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 's', ' ', 'e', 'n', 'd', 'o', 'r', 's', 'e', 'm', 'e', 'n', 't', ' ', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'e', ' ', 'a', 'c', 'c', 'u', 's', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'r', 'o', 'a', 'd', 'e', 'r', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'u', 's', 'e', 'd', ' ', 't', 'o', ' ', 'b', 'e', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'a', 'k', 'i', 'n', ' ', 't', 'o', ' ', 'a', 'l', 'l', 'i', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'e', 'n', 'n', 'e', 's', 's', 'e', 'a', 'n', ' ', 'w', 'a', 's', ' ', 'o', 'n', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', \"'\", 's', ' ', 's', 'h', 'o', 'r', 't', ' ', 'l', 'i', 's', 't', ' ', 'f', 'o', 'r', ' ', 'v', 'i', 'c', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'a', 'n', 'd', ' ', 's', 'e', 'c', 'r', 'e', 't', 'a', 'r', 'y', ' ', 'o', 'f', ' ', 's', 't', 'a', 't', 'e', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'o', 'p', 'y', 'r', 'i', 'g', 'h', 't', ' ', 'g', 'e', 't', 't', 'y', ' ', 'i', 'm', 'a', 'g', 'e', 's', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'a', 'p', 't', 'i', 'o', 'n', ' ', 'b', 'o', 'b', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'a', 't', ' ', 't', 'r', 'u', 'm', 'p', ' ', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', ' ', 'r', 'a', 'l', 'l', 'y', ' ', 'i', 'n', ' ', 'j', 'u', 'l', 'y', ' ', '2', '0', '1', '6', ' ', 't', 'h', 'o', 's', 'e', ' ', 'd', 'a', 'y', 's', ' ', 'a', 'r', 'e', ' ', 's', 'e', 'e', 'm', 'i', 'n', 'g', 'l', 'y', ' ', 'v', 'e', 'r', 'y', ' ', 'm', 'u', 'c', 'h', ' ', 'o', 'v', 'e', 'r', ' ', 'n', 'o', 'w', ' ', 'a', 'n', 'd', ' ', 'i', 't', ' ', \"'\", 's', ' ', 'n', 'o', 't', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 'a', 'n', 'y', 'w', 'h', 'e', 'r', 'e', ' ', 'a', 'n', 'y', 't', 'i', 'm', 'e', ' ', 's', 'o', 'o', 'n', ' ', 'a', 'l', 't', 'h', 'o', 'u', 'g', 'h', ' ', 'h', 'e', ' ', \"'\", 's', ' ', 'n', 'o', 't', ' ', 'r', 'u', 'n', 'n', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 'r', 'e', ' ', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ' ', ',', ' ', 'h', 'e', ' ', \"'\", 'l', 'l', ' ', 'b', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 's', 'e', 'n', 'a', 't', 'e', ' ', ',', ' ', 'c', 'h', 'a', 'i', 'r', 'i', 'n', 'g', ' ', 'a', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'c', 'o', 'm', 'm', 'i', 't', 't', 'e', 'e', ' ', ',', ' ', 'u', 'n', 't', 'i', 'l', ' ', 'j', 'a', 'n', 'u', 'a', 'r', 'y', ' ', '2', '0', '1', '9', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'm', 'a', 'r', 'g', 'i', 'n', ' ', 'f', 'o', 'r', ' ', 's', 'u', 'c', 'c', 'e', 's', 's', ' ', 'i', 'n', ' ', 't', 'h', 'a', 't', ' ', 'c', 'h', 'a', 'm', 'b', 'e', 'r', ' ', 'i', 's', ' ', 'r', 'a', 'z', 'o', 'r', ' ', 't', 'h', 'i', 'n', ' ', 'i', 'f', ' ', 'd', 'e', 'm', 'o', 'c', 'r', 'a', 't', 's', ' ', 'c', 'a', 'n', ' ', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', ' ', 't', 'o', ' ', 's', 't', 'a', 'n', 'd', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 'i', 'n', ' ', 'o', 'p', 'p', 'o', 's', 'i', 't', 'i', 'o', 'n', ' ', ',', ' ', 'h', 'e', ' ', 'c', 'a', 'n', ' ', 'a', 'f', 'f', 'o', 'r', 'd', ' ', 't', 'o', ' ', 'l', 'o', 's', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'w', 'o', ' ', 'v', 'o', 't', 'e', 's', ' ', 'o', 'u', 't', ' ', 'o', 'f', ' ', '5', '2', ' ', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'c', 'a', 'n', ' ', 's', 'e', 'n', 'a', 't', 'o', 'r', 's', ' ', 't', 'h', 'a', 't', ' ', \"'\", 's', ' ', 'w', 'h', 'y', ' ', 'h', 'e', 'a', 'l', 't', 'h', 'c', 'a', 'r', 'e', ' ', 'r', 'e', 'f', 'o', 'r', 'm', ' ', 'c', 'o', 'l', 'l', 'a', 'p', 's', 'e', 'd', ' ', 'i', 'n', ' ', 'j', 'u', 'l', 'y', ' ', 'a', 'n', 'd', ' ', 'i', 't', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'b', 'a', 'd', ' ', 'n', 'e', 'w', 's', ' ', 'f', 'o', 'r', ' ', 't', 'a', 'x', ' ', 'e', 'f', 'f', 'o', 'r', 't', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'e', 'r', 'e', ' ', 'o', 'n', ' ', 'o', 'u', 't', ' ', ',', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'n', \"'\", 't', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'd', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'a', 'n', 'y', ' ', 'f', 'a', 'v', 'o', 'u', 'r', 's', ' ', 'l', 'o', 'o', 'k', ' ', ',', ' ', 'e', 'x', 'c', 'e', 'p', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'f', 'e', 'w', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', ',', ' ', 't', 'h', 'e', ' ', 'v', 'a', 's', 't', ' ', 'm', 'a', 'j', 'o', 'r', 'i', 't', 'y', ' ', 'o', 'f', ' ', 'o', 'u', 'r', ' ', 'c', 'a', 'u', 'c', 'u', 's', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 's', ' ', 'w', 'h', 'a', 't', ' ', 'w', 'e', ' ', \"'\", 'r', 'e', ' ', 'd', 'e', 'a', 'l', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'e', 'r', 'e', ' ', 'f', 'r', 'u', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'i', 'n', ' ', 'c', 'o', 'n', 'g', 'r', 'e', 's', 's', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 'g', 'r', 'o', 'w', 'i', 'n', 'g', ' ', 'o', 'v', 'e', 'r', ' ', 'w', 'h', 'a', 't', ' ', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'c', 'a', 'n', 's', ' ', 'f', 'e', 'e', 'l', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'i', 'n', 'a', 'b', 'i', 'l', 'i', 't', 'y', ' ', 't', 'o', ' ', 'f', 'o', 'c', 'u', 's', ' ', 'o', 'n', ' ', 'a', 'd', 'v', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'a', 'g', 'e', 'n', 'd', 'a', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 's', 'h', 'a', 'r', 'p', 'l', 'y', ' ', 'd', 'i', 'v', 'i', 'd', 'e', 'd', ' ', 'p', 'a', 'r', 't', 'y', ' ', 't', 'o', ' ', 'c', 'o', 'm', 'e', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 'o', 'n', ' ', 'p', 'l', 'a', 'n', 's', ' ', 't', 'o', ' ', 'r', 'e', 'p', 'e', 'a', 'l', ' ', 'o', 'b', 'a', 'm', 'a', 'c', 'a', 'r', 'e', ' ', ',', ' ', 'r', 'e', 'f', 'o', 'r', 'm', ' ', 't', 'a', 'x', 'e', 's', ' ', 'o', 'r', ' ', 'b', 'o', 'o', 's', 't', ' ', 'i', 'n', 'f', 'r', 'a', 's', 't', 'r', 'u', 'c', 't', 'u', 'r', 'e', ' ', 's', 'p', 'e', 'n', 'd', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'i', 'n', 'g', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 's', 'o', ' ', 'w', 'h', 'e', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 's', 't', 'i', 'r', 's', ' ', 'u', 'p', ' ', 'u', 'n', 'r', 'e', 'l', 'a', 't', 'e', 'd', ' ', 'c', 'o', 'n', 't', 'r', 'o', 'v', 'e', 'r', 's', 'i', 'e', 's', ' ', 'o', 'n', ' ', 'a', ' ', 's', 'e', 'e', 'm', 'i', 'n', 'g', 'l', 'y', ' ', 'd', 'a', 'i', 'l', 'y', ' ', 'b', 'a', 's', 'i', 's', ' ', 'm', 'a', 'k', 'e', 's', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'g', 'i', 'f', 't', 's', ' ', 'h', 'a', 's', ' ', 'b', 'e', 'e', 'n', ' ', 'h', 'i', 's', ' ', 'a', 'b', 'i', 'l', 'i', 't', 'y', ' ', 't', 'o', ' ', 's', 'h', 'a', 'k', 'e', ' ', 'o', 'f', 'f', ' ', 'n', 'e', 'g', 'a', 't', 'i', 'v', 'e', ' ', 's', 't', 'o', 'r', 'i', 'e', 's', ' ', 'b', 'y', ' ', 'q', 'u', 'i', 'c', 'k', 'l', 'y', ' ', 'm', 'o', 'v', 'i', 'n', 'g', ' ', 'o', 'n', ' ', 't', 'o', ' ', 'a', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 'j', 'e', 'c', 't', ' ', 't', 'h', 'a', 't', ' ', 'w', 'o', 'r', 'k', 'e', 'd', ' ', 'b', 'r', 'i', 'l', 'l', 'i', 'a', 'n', 't', 'l', 'y', ' ', 'd', 'u', 'r', 'i', 'n', 'g', ' ', 'h', 'i', 's', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', ' ', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', ' ', ',', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', \"'\", 's', ' ', 'l', 'e', 's', 's', ' ', 'e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'e', ' ', 'd', 'u', 'r', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'g', 'i', 's', 'l', 'a', 't', 'i', 'v', 'e', ' ', 's', 'l', 'o', 'w', ' ', 'g', 'r', 'i', 'n', 'd', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'o', 'p', 'y', 'r', 'i', 'g', 'h', 't', ' ', 'g', 'e', 't', 't', 'y', ' ', 'i', 'm', 'a', 'g', 'e', 's', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'a', 'p', 't', 'i', 'o', 'n', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'n', 'f', 'i', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'h', 'e', 'a', 'r', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 's', 'e', 'c', 'r', 'e', 't', 'a', 'r', 'y', ' ', 'o', 'f', ' ', 's', 't', 'a', 't', 'e', ' ', 'r', 'e', 'x', ' ', 't', 'i', 'l', 'l', 'e', 'r', 's', 'o', 'n', ' ', 'f', 'o', 'r', ' ', 'm', 'o', 'n', 't', 'h', 's', ' ', ',', ' ', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'c', 'a', 'n', 's', ' ', 'i', 'n', ' ', 'c', 'o', 'n', 'g', 'r', 'e', 's', 's', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'e', 'e', 'n', ' ', 'g', 'r', 'u', 'm', 'b', 'l', 'i', 'n', 'g', ' ', 'a', 'b', 'o', 'u', 't', ' ', 't', 'h', 'i', 's', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'c', 'k', 'g', 'r', 'o', 'u', 'n', 'd', ' ', 'a', 'n', 'd', ' ', 'a', 'm', 'o', 'n', 'g', ' ', 't', 'h', 'e', 'm', 's', 'e', 'l', 'v', 'e', 's', ' ', 'o', 'c', 'c', 'a', 's', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', ',', ' ', 's', 'o', 'm', 'e', 'o', 'n', 'e', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'r', ' ', 'm', 'c', 'c', 'o', 'n', 'n', 'e', 'l', 'l', ' ', 'w', 'i', 'l', 'l', ' ', 'l', 'a', 'm', 'e', 'n', 't', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'd', 'o', 'e', 's', ' ', 'n', \"'\", 't', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 't', 'h', 'e', ' ', 's', 'e', 'n', 'a', 't', 'e', ' ', 'w', 'o', 'r', 'k', 's', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'h', 'a', 's', ' ', 'n', 'o', 'w', ' ', 's', 't', 'a', 't', 'e', 'd', ' ', 'i', 't', ' ', 'l', 'o', 'u', 'd', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'e', 'a', 'r', ' ', 'a', 'n', 'd', ' ', ',', ' ', 'w', 'h', 'a', 't', ' ', \"'\", 's', ' ', 'm', 'o', 'r', 'e', ' ', ',', ' ', 'h', 'e', ' ', 's', 'a', 'y', 's', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'a', 'g', 'r', 'e', 'e', 's', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'i', 'm', ' ', 't', 'h', 'e', 'y', ' ', \"'\", 'v', 'e', ' ', 'k', 'e', 'p', 't', ' ', 's', 'i', 'l', 'e', 'n', 't', ' ', 'u', 'n', 't', 'i', 'l', ' ', 'n', 'o', 'w', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', 'y', ' ', 's', 't', 'i', 'l', 'l', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'p', 'a', 's', 's', ' ', 'c', 'o', 'n', 's', 'e', 'r', 'v', 'a', 't', 'i', 'v', 'e', ' ', 'l', 'e', 'g', 'i', 's', 'l', 'a', 't', 'i', 'o', 'n', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'c', 'a', 'n', ' ', 's', 'i', 'g', 'n', ' ', 'o', 'r', ' ', 'f', 'e', 'a', 'r', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', \"'\", 's', ' ', 'l', 'e', 'g', 'i', 'o', 'n', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'a', 'c', 'k', ' ', 'a', ' ', 'p', 'r', 'i', 'm', 'a', 'r', 'y', ' ', 'c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', ' ', 'n', 'e', 'x', 't', ' ', 'y', 'e', 'a', 'r', ' ', 'o', 'r', ' ', 's', 't', 'a', 'y', ' ', 'h', 'o', 'm', 'e', ' ', 'd', 'u', 'r', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 'l', ' ', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'i', 'f', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'l', 'c', 'u', 'l', 'u', 's', ' ', 'e', 'v', 'e', 'r', ' ', 'c', 'h', 'a', 'n', 'g', 'e', 's', ' ', 'i', 'f', ' ', 'i', 't', ' ', 'b', 'e', 'c', 'o', 'm', 'e', 's', ' ', 'r', 'i', 's', 'k', 'i', 'e', 'r', ' ', 't', 'o', ' ', 's', 't', 'a', 'y', ' ', 's', 'i', 'l', 'e', 'n', 't', ' ', 't', 'h', 'a', 'n', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'o', 'u', 't', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'i', 'n', ' ', 'r', 'e', 'a', 'l', ' ', 't', 'r', 'o', 'u', 'b', 'l', 'e', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 't', 'h', 'i', 'n', 'k', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 's', 'o', 'm', 'e', ' ', 'k', 'i', 'n', 'd', ' ', 'o', 'f', ' ', \"'\", 'g', 'o', 'o', 'd', ' ', 'c', 'o', 'p', ' ', ',', ' ', 'b', 'a', 'd', ' ', 'c', 'o', 'p', \"'\", ' ', 'a', 'c', 't', ' ', 'u', 'n', 'd', 'e', 'r', 'w', 'a', 'y', ' ', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'a', 't', ' ', \"'\", 's', ' ', 'j', 'u', 's', 't', ' ', 'n', 'o', 't', ' ', 't', 'r', 'u', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'a', 'g', 'a', 'i', 'n', ' ', ',', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', 'h', 'a', 's', ' ', 'a', 'p', 'p', 'e', 'a', 'r', 'e', 'd', ' ', 't', 'o', ' ', 'u', 'n', 'd', 'e', 'r', 'c', 'u', 't', ' ', 's', 'e', 'c', 'r', 'e', 't', 'a', 'r', 'y', ' ', 'o', 'f', ' ', 's', 't', 'a', 't', 'e', ' ', 'r', 'e', 'x', ' ', 't', 'i', 'l', 'l', 'e', 'r', 's', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'o', 't', 'h', 'e', 'r', 's', ' ', 'i', 'n', ' ', 'h', 'i', 's', ' ', 'a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'h', 'o', ' ', 'a', 'r', 'e', ' ', 'a', 't', 't', 'e', 'm', 'p', 't', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'u', 's', 'e', ' ', 's', 'o', 'f', 't', ' ', 'd', 'i', 'p', 'l', 'o', 'm', 'a', 'c', 'y', ' ', 't', 'o', ' ', 'd', 'e', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'r', 'a', 'n', 'g', 'e', ' ', 'o', 'f', ' ', 'i', 'n', 't', 'e', 'r', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'c', 'r', 'i', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'w', 'a', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 't', 'a', 'l', 'i', 'b', 'a', 'n', ' ', 'i', 'n', ' ', 'a', 'f', 'g', 'h', 'a', 'n', 'i', 's', 't', 'a', 'n', ' ', ',', ' ', 'i', 'r', 'a', 'n', ' ', \"'\", 's', ' ', 'c', 'o', 'm', 'p', 'l', 'i', 'a', 'n', 'c', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'm', 'u', 'l', 't', 'i', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'n', 'u', 'c', 'l', 'e', 'a', 'r', ' ', 'a', 'g', 'r', 'e', 'e', 'm', 'e', 'n', 't', ' ', ',', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'g', 'o', 'i', 'n', 'g', ' ', 'd', 'i', 's', 'p', 'u', 't', 'e', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'q', 'a', 't', 'a', 'r', ' ', 'a', 'n', 'd', ' ', 'i', 't', 's', ' ', 'p', 'e', 'r', 's', 'i', 'a', 'n', ' ', 'g', 'u', 'l', 'f', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ' ', ',', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'r', 'e', 's', 't', ' ', 'i', 'n', ' ', 'v', 'e', 'n', 'e', 'z', 'u', 'e', 'l', 'a', ' ', 'a', 'n', 'd', ' ', ',', ' ', 'm', 'o', 's', 't', ' ', 'r', 'e', 'c', 'e', 'n', 't', 'l', 'y', ' ', ',', ' ', 'n', 'o', 'r', 't', 'h', ' ', 'k', 'o', 'r', 'e', 'a', ' ', \"'\", 's', ' ', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', 'd', ' ', 'b', 'a', 'l', 'l', 'i', 's', 't', 'i', 'c', ' ', 'm', 'i', 's', 's', 'i', 'l', 'e', ' ', 't', 'e', 's', 't', 's', ' ', 'h', 'a', 'v', 'e', ' ', 'a', 'l', 'l', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'h', 'e', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'o', 'f', 'f', 'h', 'a', 'n', 'd', ' ', 'r', 'e', 'm', 'a', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 't', 'w', 'i', 't', 't', 'e', 'r', ' ', 'i', 'n', 'v', 'e', 'c', 't', 'i', 'v', 'e', ' ', 's', 'o', 'm', 'e', ' ', 'a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'd', 'e', 'f', 'e', 'n', 'd', 'e', 'r', 's', ' ', 'h', 'a', 'v', 'e', ' ', 's', 'a', 'i', 'd', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'l', 'l', ' ', 'a', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', \"'\", 's', ' ', 's', 't', 'r', 'a', 't', 'e', 'g', 'y', ' ', 'a', 'n', ' ', 'u', 'p', 'd', 'a', 't', 'e', 'd', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'n', 'i', 'x', 'o', 'n', ' ', 'e', 'r', 'a', ' ', 'm', 'a', 'd', 'm', 'a', 'n', ' ', 't', 'h', 'e', 'o', 'r', 'y', ' ', ',', ' ', 'i', 'n', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'f', 'o', 'r', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'r', 's', 'a', 'r', 'i', 'e', 's', ' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 'w', 'a', 'y', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', 'y', ' ', 'f', 'e', 'a', 'r', ' ', 'a', 'n', ' ', 'u', 'n', 'p', 'r', 'e', 'd', 'i', 'c', 't', 'a', 'b', 'l', 'e', ' ', 'u', 's', ' ', 'l', 'e', 'a', 'd', 'e', 'r', ' ', \"'\", 's', ' ', 'a', 'c', 't', 'i', 'o', 'n', 's', ' ', 'm', 'r', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'n', \"'\", 't', ' ', 'b', 'u', 'y', 'i', 'n', 'g', ' ', 'i', 't', ' ', 't', 'h', 'e', 'r', 'e', ' ', \"'\", 's', ' ', 'n', 'o', ' ', 's', 't', 'r', 'a', 't', 'e', 'g', 'y', ' ', ',', ' ', 'h', 'e', ' ', 's', 'a', 'y', 's', ' ', ',', ' ', 'j', 'u', 's', 't', ' ', 't', 'h', 'e', ' ', 'p', 'o', 's', 's', 'i', 'b', 'i', 'l', 'i', 't', 'y', ' ', 'o', 'f', ' ', 'c', 'h', 'a', 'o', 's', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'h', 'e', ' ', 'h', 'o', 'p', 'e', 's', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', \"'\", 's', ' ', 's', 'e', 'n', 'i', 'o', 'r', ' ', 'a', 'd', 'v', 'i', 's', 'e', 'r', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'a', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'a', 'v', 'o', 'i', 'd', ' ', 'i', ' ', 'k', 'n', 'o', 'w', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'f', 'a', 'c', 't', ' ', 't', 'h', 'a', 't', ' ', 'e', 'v', 'e', 'r', 'y', ' ', 's', 'i', 'n', 'g', 'l', 'e', ' ', 'd', 'a', 'y', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'w', 'h', 'i', 't', 'e', ' ', 'h', 'o', 'u', 's', 'e', ' ', ',', ' ', 'i', 't', ' ', \"'\", 's', ' ', 'a', ' ', 's', 'i', 't', 'u', 'a', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'r', 'y', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'c', 'o', 'n', 't', 'a', 'i', 'n', ' ', 'h', 'i', 'm', ' ', 't', 'h', 'e', 'r', 'e', ' ', \"'\", 's', ' ', 'n', 'o', 'w', ' ', 'a', ' ', 'g', 'r', 'o', 'w', 'i', 'n', 'g', ' ', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'j', 'o', 'h', 'n', ' ', 'k', 'e', 'l', 'l', 'y', ' ', 'f', 'a', 'c', 'e', ' ', 'p', 'a', 'l', 'm', ' ', 'p', 'h', 'o', 't', 'o', 's', ' ', 't', 'h', 'a', 't', ' ', 's', 'e', 'r', 'v', 'e', ' ', 'a', 's', ' ', 'a', ' ', 't', 'e', 's', 't', 'a', 'm', 'e', 'n', 't', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'o', 'f', ' ', 's', 't', 'a', 'f', 'f', ' ', \"'\", 's', ' ', 'r', 'e', 'p', 'o', 'r', 't', 'e', 'd', ' ', 'f', 'r', 'u', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'a', 't', ' ', 'd', 'e', 'a', 'l', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'm', 'r', ' ', 't', 'r', 'u', 'm', 'p', ' ', 'g', 'o', 'e', 's', ' ', 'o', 'f', 'f', ' ', 's', 'c', 'r', 'i', 'p', 't', ' ', 't', 'o', ' ', 'p', 'r', 'a', 'i', 's', 'e', ' ', 't', 'o', 'r', 'c', 'h', ' ', 'b', 'e', 'a', 'r', 'i', 'n', 'g', ' ', 'w', 'h', 'i', 't', 'e', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'i', 's', 't', 's', ' ', 'a', 't', ' ', 'a', ' ', 'r', 'a', 'l', 'l', 'y', ' ', 'i', 'n', ' ', 'c', 'h', 'a', 'r', 'l', 'o', 't', 't', 'e', 's', 'v', 'i', 'l', 'l', 'e', ' ', ',', ' ', 'a', 'n', 'd', ' ', 'm', 'r', ' ', 'k', 'e', 'l', 'l', 'y', ' ', 'i', 's', ' ', 'c', 'a', 'p', 't', 'u', 'r', 'e', 'd', ' ', 'c', 'l', 'o', 's', 'i', 'n', 'g', ' ', 'h', 'i', 's', ' ', 'e', 'y', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'r', 'u', 'b', 'b', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'a', 'r', 'c', 'h', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'n', 'o', 's', 'e', ' ', ',', ' ', 'a', 's', ' ', 'i', 'f', ' ', 'a', 't', 't', 'e', 'm', 'p', 't', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 't', 'a', 'v', 'e', ' ', 'o', 'f', 'f', ' ', 'a', ' ', 'm', 'i', 'g', 'r', 'a', 'i', 'n', 'e', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'o', 'p', 'y', 'r', 'i', 'g', 'h', 't', ' ', 'r', 'e', 'u', 't', 'e', 'r', 's', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'c', 'a', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'h', 'i', 't', 'e', ' ', 'h', 'o', 'u', 's', 'e', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'o', 'f', ' ', 's', 't', 'a', 'f', 'f', ' ', 'j', 'o', 'h', 'n', ' ', 'k', 'e', 'l', 'l', 'y', ' ', 'l', 'o', 'o', 'k', 's', ' ', 'o', 'n', ' ', 'a', 's', ' ', 'u', 's', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'd', 'o', 'n', 'a', 'l', 'd', ' ', 't', 'r', 'u', 'm', 'p', ' ', 's', 'p', 'e', 'a', 'k', 's', ' ', 'a', 't', ' ', 'a', ' ', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', ' ', 'r', 'a', 'l', 'l', 'y', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 'c', 'a', 'l', 'l', 's', ' ', 'n', 'o', 'r', 't', 'h', ' ', 'k', 'o', 'r', 'e', 'a', 'n', ' ', 'l', 'e', 'a', 'd', 'e', 'r', 's', ' ', 'c', 'r', 'i', 'm', 'i', 'n', 'a', 'l', 's', ' ', 'i', 'n', ' ', 'a', ' ', 's', 'p', 'e', 'e', 'c', 'h', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'i', 't', 'e', 'd', ' ', 'n', 'a', 't', 'i', 'o', 'n', 's', ' ', ',', ' ', 'a', 'n', 'd', ' ', 'm', 'r', ' ', 'k', 'e', 'l', 'l', 'y', ' ', 's', 't', 'r', 'a', 'i', 'g', 'h', 't', ' ', 'u', 'p', ' ', 'b', 'u', 'r', 'i', 'e', 's', ' ', 'h', 'i', 's', ' ', 'f', 'a', 'c', 'e', ' ', 'i', 'n', ' ', 'h', 'i', 's', ' ', 'h', 'a', 'n', 'd', 's', ' ', 't', 'h', 'e', ' ', 'w', 'h', 'i', 't', 'e', ' ', 'h', 'o', 'u', 's', 'e', ' ', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's', ' ', 't', 'e', 'a', 'm', ' ', 'i', 's', ' ', 'o', 'f', 't', 'e', 'n', ' ', 'l', 'e', 'f', 't', ' ', 's', 'c', 'r', 'a', 'm', 'b', 'l', 'i', 'n', 'g', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'l', 'a', 'i', 'n', ' ', 'o', 'r', ' ', 'r', 'e', 'f', 'r', 'a', 'm', 'e', ' ', 'a', 'n', ' ', 'i', 'n', 'd', 'e', 'l', 'i', 'c', 'a', 't', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', ' ', 'j', 'o', 'k', 'e', ' ', 'o', 'r', ' ', 'r', 'e', 'm', 'a', 'r', 'k', ' ', 't', 'h', 'a', 't', ' ', 'd', 'i', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'c', 'o', 'n', 't', 'r', 'a', 'd', 'i', 'c', 't', 's', ' ', 'w', 'h', 'a', 't', ' ', 'w', 'a', 's', ' ', 'u', 'n', 't', 'i', 'l', ' ', 't', 'h', 'e', 'n', ' ', 't', 'h', 'e', ' ', 'o', 'f', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'l', 'i', 'n', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 't', 'h', 'o', 'u', 'g', 'h', ' ', 'm', 'r', ' ', 'k', 'e', 'l', 'l', 'y', ' ', 'h', 'a', 's', ' ', 'b', 'r', 'o', 'u', 'g', 'h', 't', ' ', 's', 'o', 'm', 'e', ' ', 'd', 'i', 's', 'c', 'i', 'p', 'l', 'i', 'n', 'e', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'w', 'e', 's', 't', ' ', 'w', 'i', 'n', 'g', ' ', 's', 't', 'a', 'f', 'f', ' ', ',', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', 's', 't', 'i', 'l', 'l', ' ', 'm', 'a', 'r', 'c', 'h', 'e', 's', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 't', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'o', 'w', 'n', ' ', 'd', 'r', 'u', 'm', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', 's', ' ', 't', 'o', ' ', 'h', 'a', 'v', 'e', ' ', 'u', 'n', 'f', 'e', 't', 't', 'e', 'r', 'e', 'd', ' ', 'a', 'c', 'c', 'e', 's', 's', ' ', 't', 'o', ' ', 'h', 'i', 's', ' ', 'p', 'h', 'o', 'n', 'e', ' ', \"'\", 's', ' ', 't', 'w', 'i', 't', 't', 'e', 'r', ' ', 'a', 'p', 'p', ' ', 'b', 'o', 'b', ' ', 'c', 'o', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'e', ' ', 'l', 'a', 't', 'e', 's', 't', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'p', 'o', 'l', 'i', 't', 'i', 'c', 'i', 'a', 'n', ' ', ',', ' ', 'j', 'o', 'u', 'r', 'n', 'a', 'l', 'i', 's', 't', ' ', ',', ' ', 's', 'p', 'o', 'r', 't', 's', ' ', 's', 't', 'a', 'r', ' ', 'o', 'r', ' ', 'c', 'e', 'l', 'e', 'b', 'r', 'i', 't', 'y', ' ', 't', 'o', ' ', 'f', 'e', 'e', 'l', ' ', 't', 'h', 'e', ' ', 'm', 'e', 'r', 'c', 'u', 'r', 'i', 'a', 'l', ' ', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ' ', \"'\", 's', ' ', 'u', 'n', 'c', 'o', 'n', 't', 'a', 'i', 'n', 'a', 'b', 'l', 'e', ' ', 'i', 'r', 'e']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-8ed7cf1a43be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'_PAD'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_UNK'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "txt = \"\"\n",
    "for saison in os.listdir(directory):\n",
    "    dir_saison = \"{}/{}\".format(directory, saison)\n",
    "    for ep in os.listdir(dir_saison):\n",
    "        adr = \"{}/{}\".format(dir_saison,ep)\n",
    "        with open(adr, 'r') as f:\n",
    "            data = f.read()\n",
    "            txt+=data\n",
    "\n",
    "            \n",
    "token = nltk.word_tokenize(txt)\n",
    "words = Counter(token)\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['_PAD','_UNK'] + words\n",
    "vocab_size = len(words)\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "word2idx[\"UNK\"]=len(word2idx)\n",
    "print(\"vocab size : \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in txt for txt in train_data]\n",
    "[w for txt.split() in train_data for w in txt]\n",
    "[x for b in a for x in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b838371b7eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-b838371b7eda>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-b838371b7eda>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "train_data = [[word2idx[w] for w in s] for s in train_data]\n",
    "dev_data = [[word2idx[w] if w in word2idx else 0 for w in s] for s in dev_data]\n",
    "test_data = [[word2idx[w] if w in word2idx else 0 for w in s] for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170, 473, 953, 448, 11, 231, 800, 3, 389, 71, 340, 141, 11, 7, 213, 13668, 98, 7, 781, 6, 28, 219, 331, 22, 3, 8, 964, 3, 10, 138, 1349, 1029, 83, 13, 30, 330, 31, 2, 67, 79, 2, 100, 29, 870, 28, 16815, 23000, 11, 28, 219, 4326, 22, 79, 3, 233, 3, 28, 918, 57, 1475, 9716, 5, 843, 15, 2621, 221, 1564, 1476, 1913, 1076, 3, 2, 4012, 6, 2, 844, 886, 405, 1467, 576, 3, 7, 13669, 12, 36, 323, 12, 192, 449, 15, 24, 93, 1076, 16816, 12, 2, 100, 30, 6528, 3, 52, 15, 2138, 4, 381, 15, 11681, 1661, 9, 93, 1076, 30, 314, 6, 2, 2316, 430, 772, 17, 28, 96, 310, 19421, 2704, 77, 6, 28, 2906, 3, 93, 1076, 271, 31, 477, 4, 3357, 49, 28, 2580, 310, 507, 118, 138, 1041, 28, 10952, 4013, 213, 295, 21, 23001, 13, 30, 7, 4406, 2, 230, 203, 29, 425, 35, 4014, 160, 878, 490, 879, 1356, 1270, 41, 2907, 22, 800, 1476, 1913, 1076, 23, 23001, 20, 185, 253, 3, 97, 242, 9, 17, 138, 2, 214, 6, 13, 3, 360, 15, 140, 1638, 16, 2, 53, 152, 145, 5, 248, 410, 2, 100, 25, 13, 146, 27, 217, 1914, 7430, 31, 2, 1564, 1476, 30, 835, 16, 2, 145, 5, 301, 37, 27, 1475, 11682, 33, 84, 138, 177, 301, 2, 100, 2450, 48, 293, 9, 27, 36, 930, 34, 177, 15, 237, 13, 3, 616, 1693, 15, 237, 13, 3, 32, 15, 237, 34, 1217, 138, 248, 29859, 22, 46, 93, 1076, 10, 3063, 48, 423, 2, 100, 10, 7, 15113, 5, 616, 1693, 13, 2, 1476, 3, 8, 1565, 3, 10, 3024, 93, 71, 30, 19422, 9, 15, 10282, 10953, 12, 28, 6528, 3, 32, 2, 8760, 10, 164, 3598, 93, 1076, 5, 2, 100, 220, 4, 26, 265, 10954, 4, 1405, 2, 23002, 17, 11, 93, 71, 30, 651, 378, 12, 1320, 100, 5, 643, 6, 109, 170, 473, 953, 448, 170, 171, 1913, 1076, 19, 71, 353, 2451, 8, 820, 299, 131, 297, 27, 4015, 172, 164, 66, 91, 5, 13, 30, 36, 72, 93, 1076, 10, 130, 3951, 7177, 995, 782, 15, 30, 36, 323, 12, 192, 449, 3, 15, 1555, 26, 8, 2, 886, 3, 29860, 7, 844, 576, 3, 418, 1442, 3025, 2, 100, 30, 5939, 12, 1037, 8, 9, 5074, 10, 23003, 5940, 60, 1152, 56, 186, 4, 637, 491, 8, 1068, 3, 15, 56, 3818, 4, 1321, 96, 64, 2908, 48, 6, 3137, 830, 3173, 9, 30, 301, 1994, 1357, 4084, 8, 820, 5, 13, 94, 26, 589, 83, 12, 394, 862, 31, 146, 11, 48, 3, 93, 1076, 10, 138, 130, 4, 84, 2, 100, 117, 13670, 268, 3, 2909, 12, 7, 327, 73, 3, 2, 3098, 1316, 6, 86, 8761, 4859, 54, 38, 329, 2642, 16, 146, 5783, 8, 618, 29, 47, 1095, 66, 54, 1340, 654, 29, 47, 2, 100, 30, 8762, 4, 1077, 11, 6338, 41, 1977, 457, 7, 5075, 4016, 331, 4, 262, 491, 11, 613, 4, 3256, 3310, 3, 1357, 1995, 43, 2756, 2110, 1822, 10, 3024, 484, 562, 69, 62, 2, 100, 29861, 45, 12626, 9717, 11, 7, 4015, 431, 2087, 528, 293, 50, 2, 2945, 46, 6, 2, 100, 30, 12627, 29, 47, 28, 1042, 4, 4327, 141, 3138, 1207, 21, 1043, 1163, 11, 4, 7, 408, 1662, 9, 836, 11683, 128, 28, 1054, 353, 3, 32, 13, 30, 488, 1527, 128, 2, 2819, 2088, 10955, 170, 473, 953, 448, 170, 171, 1076, 19, 2, 8763, 1933, 12, 643, 6, 109, 2678, 1144, 12, 468, 3, 1340, 8, 618, 25, 47, 29862, 49, 22, 8, 2, 2946, 5, 344, 871, 5457, 3, 879, 72, 93, 4328, 40, 16817, 9, 2, 100, 237, 138, 1059, 106, 2, 886, 1218, 93, 1076, 29, 91, 1888, 13, 5076, 5, 567, 5, 3, 54, 30, 44, 3, 15, 181, 665, 616, 7178, 16, 82, 37, 837, 1740, 4241, 418, 91, 123, 37, 144, 733, 4, 398, 666, 1279, 9, 2, 100, 56, 288, 43, 1468, 93, 71, 30, 29863, 40, 108, 7, 2270, 1247, 201, 74, 43, 890, 153, 128, 2, 582, 449, 60, 9, 23004, 436, 1208, 60, 13, 2705, 19423, 4, 890, 4241, 92, 1322, 48, 93, 71, 40, 26, 8, 390, 2731, 7, 321, 6, 73, 149, 9, 58, 10, 77, 851, 6, 23005, 5941, 3, 589, 29864, 470, 4860, 3, 32, 9, 30, 75, 36, 930, 79, 5, 238, 3, 93, 71, 29, 536, 4, 13671, 643, 6, 109, 2678, 1144, 5, 500, 8, 28, 432, 39, 27, 3819, 4, 240, 5077, 7179, 4, 256, 16, 7, 1412, 6, 309, 11684, 2, 278, 98, 2, 5321, 8, 2366, 3, 332, 30, 3655, 16, 2, 8034, 430, 772, 3, 2, 1950, 3257, 207, 2982, 5, 68, 23006, 3394, 13672, 3, 2, 8035, 8, 3656, 5, 3, 110, 801, 3, 178, 322, 30, 1293, 5639, 1469, 2512, 25, 50, 47, 2, 1248, 6, 2, 100, 30, 23007, 2820, 5, 213, 23008, 77, 432, 2947, 25, 24, 22, 10, 50, 7, 202, 6, 93, 71, 30, 1741, 35, 3358, 1394, 6, 2, 5640, 955, 23009, 1809, 3, 8, 52, 2, 100, 909, 12628, 4, 381, 156, 123, 37, 1468, 35, 15114, 80, 707, 30, 1511, 93, 1076, 10, 138, 2407, 13, 58, 30, 87, 1741, 3, 15, 181, 3, 75, 2, 2534, 6, 3174, 52, 15, 1850, 93, 71, 30, 984, 6141, 40, 26, 384, 4, 1443, 33, 177, 12, 7, 239, 9, 283, 614, 160, 19, 2, 230, 203, 3, 13, 30, 7, 923, 6, 544, 4, 3175, 82, 58, 30, 91, 7, 1095, 1823, 6, 569, 3395, 508, 5078, 175, 9, 1824, 18, 7, 10283, 4, 2, 609, 6, 660, 30, 409, 5783, 19, 2642, 16, 2, 100, 93, 71, 985, 141, 5079, 4, 4159, 19424, 5322, 230, 8374, 19, 7, 2451, 8, 5080, 3, 5, 93, 3395, 10, 2558, 4473, 28, 1996, 5, 15115, 2, 9195, 6, 28, 7180, 3, 18, 60, 3819, 4, 19425, 141, 7, 23010, 170, 473, 135, 170, 171, 230, 203, 609, 6, 660, 569, 3395, 727, 11, 18, 80, 100, 389, 71, 3139, 19, 7, 353, 2451, 2, 100, 1203, 178, 669, 796, 4017, 8, 7, 1038, 4, 2, 150, 807, 3, 5, 93, 3395, 1534, 45, 29865, 28, 508, 8, 28, 1090, 2, 230, 203, 1789, 124, 10, 461, 243, 9196, 4, 435, 4, 2408, 43, 29866, 35, 29867, 1054, 5942, 43, 8375, 9, 1358, 13673, 54, 17, 418, 140, 2, 489, 432, 324, 125, 360, 93, 3395, 29, 1047, 77, 4962, 4, 2, 768, 2271, 660, 3, 2, 100, 144, 16818, 4, 2, 875, 6, 28, 219, 11685, 5, 1273, 4, 25, 16819, 737, 4, 28, 1012, 30, 213, 1825, 1913, 1076, 10, 96, 2, 776, 591, 5784, 3, 1826, 3, 254, 802, 43, 2165, 4, 654, 2, 16820, 100, 30, 29868, 11686]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(L_CBOW_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.l1 = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs = th.sum(inputs, dim=0)\n",
    "        out = th.sigmoid(self.l1(inputs))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(C_CBOW_classifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv1d(20, 50, 5, 1)\n",
    "        self.dense1 = nn.Linear(450, 250)\n",
    "        self.dense2 = nn.Linear(250, 64)\n",
    "        self.dense3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embeddings(x)\n",
    "        x = th.sum(x, dim=0)\n",
    "        x = x.view(1, 1,-1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2, 2)\n",
    "        x = x.view(-1, 450)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        return th.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lr = 1e-2\n",
    "l_cbow = L_CBOW_classifier(len(word2idx), 50)\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "dev_accuracies = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(model, max_epochs=20):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr =lr, weight_decay=1e-4)\n",
    "#     model.train()\n",
    "    idx_train = np.arange(len(train_data))\n",
    "    idx_dev = np.arange(len(dev_data))\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "        \n",
    "        np.random.shuffle(idx_train)\n",
    "        np.random.shuffle(idx_dev)\n",
    "        n=0\n",
    "        for i in idx_train:\n",
    "            s = th.tensor(train_data[i])\n",
    "            y = train_labels[i]\n",
    "            n+=1\n",
    "            label = th.tensor([y])\n",
    "            pred = model(s)\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            train_mean_loss+=loss.item()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "            \n",
    "            if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                train_accuracy+=1\n",
    "                \n",
    "            if(n%250==0):\n",
    "                print(\"step : {}/{} \".format(n, len(train_data)))\n",
    "                print(\"step accuracy : \", train_accuracy/n)\n",
    "                print(\"step loss : \", train_mean_loss/n)\n",
    "                \n",
    "        ###Dev test\n",
    "        for i in idx_dev:\n",
    "            s = th.tensor(dev_data[i])\n",
    "            y = dev_labels[i]\n",
    "            label = th.tensor([y])\n",
    "            pred = model(s)\n",
    "            loss = loss_fn(pred, label)\n",
    "            dev_mean_loss+=loss.item()\n",
    "            if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                dev_accuracy+=1\n",
    "        train_accuracies.append(train_accuracy/len(train_data))\n",
    "        train_losses.append(train_mean_loss/len(train_data))\n",
    "        dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        \n",
    "        \n",
    "        print(\"EPOCH {}\".format(e+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-9f97ee0ae5b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_cbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_cbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-d0e277487ff0>\u001b[0m in \u001b[0;36mtrain_cbow\u001b[1;34m(model, max_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-f8b3ebdf4ab1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "train_cbow(l_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, test_labels):\n",
    "    acc = 0\n",
    "    test_mean_loss = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "            s = th.tensor(test_data[i])\n",
    "            y = test_labels[i]\n",
    "            label = th.tensor([y])\n",
    "            pred = model(s)\n",
    "            loss = loss_fn(pred, label)\n",
    "            test_mean_loss+=loss.item()\n",
    "            if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                acc+=1\n",
    "    print(\"Test accuracy : \", acc/len(test_data))\n",
    "    print(\"Test mean loss : \", test_mean_loss/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.9975124378109452\n",
      "Test mean loss :  0.00988722845005508\n"
     ]
    }
   ],
   "source": [
    "test(l_cbow, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un classifieur RNN classique (LSTM ou GRU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, vocab_size, embedding_dim, rnn_dropout, is_lstm=False):\n",
    "        super(RNN_classifier, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_lstm = is_lstm\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if is_lstm:\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout)\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(0)\n",
    "        embeds = self.embedding(inputs).unsqueeze(0)\n",
    "#         embeds = embeds.view(batch_size,embeds.shape[0], embeds.shape[1])\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_size)\n",
    "        \n",
    "        out = self.dropout(rnn_out)\n",
    "        out = self.fc(out)\n",
    "        out = th.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out[-1], hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_lstm:\n",
    "            hidden = (weight.new(self.nb_cells, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.nb_cells, batch_size, self.hidden_size).zero_())\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "            \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_classifier(\n",
      "  (embedding): Embedding(45809, 10)\n",
      "  (rnn): GRU(10, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##Hyper-paramètres\n",
    "nb_cells = 1\n",
    "hidden_size = 32\n",
    "embedding_dim = 10\n",
    "learning_rate = 1e-2\n",
    "fn = nn.BCELoss()\n",
    "m = RNN_classifier(nb_cells, hidden_size, vocab_size, embedding_dim, 0.0)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, batch_size,lr, max_epochs=10, conv=False):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    best_dict = model.state_dict()\n",
    "    best_acc = 0\n",
    "    for i in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "#         train_batches = get_batches(batch_size, train_data, train_labels)\n",
    "#         dev_batches = get_batches(batch_size, dev_data, dev_labels)\n",
    "        h = model.init_hidden(batch_size)\n",
    "        n = 0 \n",
    "\n",
    "        for x, y in zip(train_data, train_labels):\n",
    "            optim.zero_grad()\n",
    "            h = h.data\n",
    "            data = th.tensor(x)\n",
    "            n+=1\n",
    "            label = th.tensor([y])\n",
    "            pred, h = model(data, h)  \n",
    "            \n",
    "            loss = fn(pred, label)\n",
    "            h = h.detach()\n",
    "            train_mean_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "\n",
    "            if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                train_accuracy+=1\n",
    "        \n",
    "            if(n%250==0):\n",
    "                print(\"step : {}/{} \".format(n, len(train_data)))\n",
    "                print(\"step accuracy : \", train_accuracy/n)\n",
    "                print(\"step loss : \", train_mean_loss/n)\n",
    "                \n",
    "        for x, y in dev_batches:\n",
    "            \n",
    "            data = th.tensor(x)\n",
    "            label = th.tensor([y])\n",
    "            pred, h = model(data, h)\n",
    "            loss = fn(pred, label)\n",
    "\n",
    "            dev_mean_loss+=loss.item()\n",
    "            \n",
    "\n",
    "            if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                dev_accuracy+=1\n",
    "        \n",
    "        if(i==0):\n",
    "            best_dict=model.state_dict()\n",
    "            best_acc=dev_accuracy/len(dev_data)\n",
    "        else:\n",
    "            if(best_acc<(dev_accuracy/len(dev_data))):\n",
    "                best_acc=dev_accuracy/len(dev_data)\n",
    "                best_dict=model.state_dict()\n",
    "                print(\"new best acc\")\n",
    "        \n",
    "        print(\"EPOCH {}\".format(i+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")\n",
    "        \n",
    "#     model.load_state_dict(best_dict)\n",
    "#     acc = test(model, test_data, conv=conv)\n",
    "#     stat_dict[model_name][4].append(acc)\n",
    "#     print(\"Accuracy on test data : \", acc)\n",
    "#     return best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbabi\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 250/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00048478447171692094\n",
      "step : 500/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00026606387300762434\n",
      "step : 750/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00019344001689417686\n",
      "step : 1000/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00015683772761667568\n",
      "step : 1250/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00013501516601554614\n",
      "step : 1500/3206 \n",
      "step accuracy :  1.0\n",
      "step loss :  0.00012109455851244395\n",
      "step : 1750/3206 \n",
      "step accuracy :  0.9965714285714286\n",
      "step loss :  0.01319653223781331\n",
      "step : 2000/3206 \n",
      "step accuracy :  0.997\n",
      "step loss :  0.011594657813240702\n",
      "step : 2250/3206 \n",
      "step accuracy :  0.9973333333333333\n",
      "step loss :  0.010330106699013818\n",
      "step : 2500/3206 \n",
      "step accuracy :  0.9976\n",
      "step loss :  0.009312117644745377\n",
      "step : 2750/3206 \n",
      "step accuracy :  0.9978181818181818\n",
      "step loss :  0.008476053057279304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-e7b44356db7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-171-0737788a025c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, batch_size, lr, max_epochs, conv)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_mean_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rnn(m, 1, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
