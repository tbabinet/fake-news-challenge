{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des données et construction des train, dev et test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##On transforme les 3 colonnes en 1 : url, titre et le texte deviennent un seul élément\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, encoding=\"utf8\", newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            txt = clean_str(row[2])\n",
    "            txt = \" \".join(txt.split(\"\\n\"))\n",
    "#             d = clean_str(row[0])+\" \"+clean_str(row[1])\n",
    "            labels.append(row[3])\n",
    "            data.append(txt)    \n",
    "    data.pop(0)\n",
    "    labels.pop(0)\n",
    "    labels = [float(l) for l in labels]\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_data(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image copyright getty images on sunday morning , donald trump went off on a twitter tirade against a member of his own party this , in itself , is n't exactly huge news it 's far from the first time the president has turned his rhetorical cannons on his own ranks this time , however , his attacks were particularly biting and personal he essentially called tennessee senator bob corker , the chair of the powerful senate foreign relations committee , a coward for not running for re election he said mr corker begged for the president 's endorsement , which he refused to give he wrongly claimed that mr corker 's support of the iranian nuclear agreement was his only political accomplishment unlike some of his colleagues , mr corker free from having to worry about his immediate political future did n't hold his tongue skip twitter post by senbobcorker it 's a shame the white house has become an adult day care center someone obviously missed their shift this morning senator bob corker \\( senbobcorker \\) october 8 , 2017 report that was n't the end of it , though he then spoke with the new york times and really let the president have it here are four choice quotes from the tennessee senator 's interview with the times and why they are particularly damning i do n't know why the president tweets out things that are not true you know he does it , everyone knows he does it , but he does you ca n't really sugarcoat this one mr corker is flat out saying the president is a liar and everyone knows it the senator , in particular , is challenging mr trump 's insistence that he unsuccessfully pleaded for his endorsement , but the accusation is much broader mr corker and the president used to be something akin to allies the tennessean was on mr trump 's short list for vice president and secretary of state image copyright getty images image caption bob corker at trump campaign rally in july 2016 those days are seemingly very much over now and it 's not like mr corker is going anywhere anytime soon although he 's not running for re election , he 'll be in the senate , chairing a powerful committee , until january 2019 the president 's margin for success in that chamber is razor thin if democrats can continue to stand together in opposition , he can afford to lose only two votes out of 52 republican senators that 's why healthcare reform collapsed in july and it could be bad news for tax efforts from here on out , mr corker is n't going to do the president any favours look , except for a few people , the vast majority of our caucus understands what we 're dealing with here frustration in congress has been growing over what republicans feel has been the president 's inability to focus on advancing their agenda getting a sharply divided party to come together on plans to repeal obamacare , reform taxes or boost infrastructure spending is challenging enough doing so when the president stirs up unrelated controversies on a seemingly daily basis makes things all the harder one of the president 's gifts has been his ability to shake off negative stories by quickly moving on to a different subject that worked brilliantly during his presidential campaign , but it 's less effective during the legislative slow grind image copyright getty images image caption corker at the confirmation hearing for secretary of state rex tillerson for months , republicans in congress have been grumbling about this in the background and among themselves occasionally , someone like mr mcconnell will lament that the president does n't understand how the senate works mr corker has now stated it loud and clear and , what 's more , he says almost everyone agrees with him they 've kept silent until now because they still hope to pass conservative legislation that the president can sign or fear mr trump 's legions will back a primary challenge next year or stay home during the general election if that calculus ever changes if it becomes riskier to stay silent than speak out mr trump will be in real trouble a lot of people think that there is some kind of 'good cop , bad cop' act underway , but that 's just not true time and again , mr trump has appeared to undercut secretary of state rex tillerson and others in his administration who are attempting to use soft diplomacy to deal with a range of international crises the war against the taliban in afghanistan , iran 's compliance with the multinational nuclear agreement , the ongoing dispute between qatar and its persian gulf neighbours , the unrest in venezuela and , most recently , north korea 's continued ballistic missile tests have all been the target of the president 's offhand remarks and twitter invective some administration defenders have said this is all a part of mr trump 's strategy an updated version of the nixon era madman theory , in which the president forces adversaries to give way because they fear an unpredictable us leader 's actions mr corker is n't buying it there 's no strategy , he says , just the possibility of chaos which he hopes mr trump 's senior advisers will be able to avoid i know for a fact that every single day at the white house , it 's a situation of trying to contain him there 's now a growing collection of john kelly face palm photos that serve as a testament to the chief of staff 's reported frustration at dealing with the president mr trump goes off script to praise torch bearing white nationalists at a rally in charlottesville , and mr kelly is captured closing his eyes and rubbing the arch of his nose , as if attempting to stave off a migraine image copyright reuters image caption white house chief of staff john kelly looks on as us president donald trump speaks at a campaign rally the president calls north korean leaders criminals in a speech to the united nations , and mr kelly straight up buries his face in his hands the white house communications team is often left scrambling to try to explain or reframe an indelicate presidential joke or remark that directly contradicts what was until then the official administration line even though mr kelly has brought some discipline to the west wing staff , the president still marches to the beat of his own drum and continues to have unfettered access to his phone 's twitter app bob corker is only the latest person politician , journalist , sports star or celebrity to feel the mercurial president 's uncontainable ire\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4009,)\n",
      "Nombre de fake news : 2137\n",
      "Nombre de news correctes : 1872\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "fake_data = data[labels==0]\n",
    "real_data = data[labels==1]\n",
    "fake_labels = labels[labels==0]\n",
    "real_labels = labels[labels==1]\n",
    "print(\"Nombre de fake news : {}\".format(len(fake_data)))\n",
    "print(\"Nombre de news correctes : {}\".format(len(real_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2137,)\n"
     ]
    }
   ],
   "source": [
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([*real_data[:int(len(real_data)*0.8)],*fake_data[:int(len(fake_data)*0.8)]])\n",
    "train_labels = np.array([*real_labels[:int(len(real_labels)*0.8)],*fake_labels[:int(len(fake_labels)*0.8)]], dtype=np.float32)\n",
    "\n",
    "dev_data = np.array([*real_data[int(len(real_data)*0.8):int(len(real_data)*0.9)], *fake_data[int(len(fake_data)*0.8):int(len(fake_data)*0.9)]])\n",
    "dev_labels = np.array([*real_labels[int(len(real_labels)*0.8):int(len(real_labels)*0.9)], *fake_labels[int(len(fake_labels)*0.8):int(len(fake_labels)*0.9)]], dtype=np.float32)\n",
    "\n",
    "test_data = np.array([*real_data[int(len(real_data)*0.9):], *fake_data[int(len(fake_data)*0.9):]])\n",
    "test_labels = np.array([*real_labels[int(len(real_labels)*0.9):], *fake_labels[int(len(fake_labels)*0.9):]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du train set : 3206\n",
      "taille du dev set : 401\n",
      "taille du test set : 402\n"
     ]
    }
   ],
   "source": [
    "print(\"taille du train set : {}\".format(len(train_data)))\n",
    "print(\"taille du dev set : {}\".format(len(dev_data)))\n",
    "print(\"taille du test set : {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  43464\n"
     ]
    }
   ],
   "source": [
    "txt = [w for txt in train_data for w in txt.split()]\n",
    "          \n",
    "# token = nltk.word_tokenize(txt)\n",
    "words = Counter(txt)\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['_PAD','_UNK'] + words\n",
    "vocab_size = len(words)\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "word2idx[\"UNK\"]=len(word2idx)\n",
    "print(\"vocab size : \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[word2idx[w] for w in news.split()] for news in train_data]\n",
    "dev_data = [[word2idx[w] if w in word2idx else 0 for w in news.split()] for news in dev_data]\n",
    "test_data = [[word2idx[w] if w in word2idx else 0 for w in news.split()] for news in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input(data, seq_len):\n",
    "    features = np.zeros((len(data), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(data):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "## on pad les input pour pouvoir créer des datasets pytorch et simplifier l'apprentissage (fonction de train plus claire, batch plus facile, etc)\n",
    "on peut tester plusieurs taille de padding : input le plus grand, moyen, min, ou arbitraire du genre 200, 500, etc. On peut aussi ainsi réduire le temps de train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seq_len = max([len(s) for s in train_data])\n",
    "# dev_seq_len = max([len(s) for s in dev_data])\n",
    "# test_seq_len = max([len(s) for s in test_data])\n",
    "\n",
    "# train_data = pad_input(train_data, train_seq_len)\n",
    "# dev_data = pad_input(dev_data, dev_seq_len)\n",
    "# test_data = pad_input(test_data, test_seq_len)\n",
    "\n",
    "seq_len = 200\n",
    "\n",
    "\n",
    "train_data = pad_input(train_data, seq_len)\n",
    "dev_data = pad_input(dev_data, seq_len)\n",
    "test_data = pad_input(test_data, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(th.from_numpy(train_data).type(th.LongTensor), th.from_numpy(train_labels))\n",
    "dev_data = TensorDataset(th.from_numpy(dev_data).type(th.LongTensor), th.from_numpy(dev_labels))\n",
    "test_data = TensorDataset(th.from_numpy(test_data).type(th.LongTensor), th.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(L_CBOW_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.l1 = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.l1.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs = th.sum(inputs, dim=1)\n",
    "        out = th.sigmoid(self.l1(inputs))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lr = 1e-2\n",
    "l_cbow = L_CBOW_classifier(len(word2idx), 50)\n",
    "cbow_train_accuracies = []\n",
    "cbow_train_losses = []\n",
    "cbow_dev_accuracies = []\n",
    "cbow_dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(model, max_epochs=20):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr =lr, weight_decay=1e-4)\n",
    "#     model.train()\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "        \n",
    "        n=0\n",
    "        for x, labels in train_loader:\n",
    "            n+=batch_size\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            train_mean_loss+=loss.item()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    train_accuracy+=1\n",
    "                \n",
    "                \n",
    "        ###Dev test\n",
    "        for x, labels in dev_loader:\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            dev_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    dev_accuracy+=1\n",
    "                    \n",
    "        cbow_train_accuracies.append(train_accuracy/len(train_data))\n",
    "        cbow_train_losses.append(train_mean_loss/len(train_data))\n",
    "        cbow_dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        cbow_dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        \n",
    "        \n",
    "        print(\"EPOCH {}\".format(e+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.6746724890829694\n",
      "Dev Accuracy :  0.7381546134663342\n",
      "Train Mean loss :  0.14222953611251643\n",
      "Dev Mean loss :  0.11491281611663742\n",
      "----------------------------------------\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.8571428571428571\n",
      "Dev Accuracy :  0.8753117206982544\n",
      "Train Mean loss :  0.043591471901947794\n",
      "Dev Mean loss :  0.043648343579727515\n",
      "----------------------------------------\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.9008109794135996\n",
      "Dev Accuracy :  0.8728179551122195\n",
      "Train Mean loss :  0.03180926841328848\n",
      "Dev Mean loss :  0.014378940301048488\n",
      "----------------------------------------\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.9373050530255771\n",
      "Dev Accuracy :  0.9027431421446384\n",
      "Train Mean loss :  0.01541062668353471\n",
      "Dev Mean loss :  0.02129922142052591\n",
      "----------------------------------------\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.9341859014348097\n",
      "Dev Accuracy :  0.9077306733167082\n",
      "Train Mean loss :  0.012504101190819154\n",
      "Dev Mean loss :  0.03781037335458242\n",
      "----------------------------------------\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.9572676232064878\n",
      "Dev Accuracy :  0.942643391521197\n",
      "Train Mean loss :  0.007443244254426215\n",
      "Dev Mean loss :  0.011213265775129237\n",
      "----------------------------------------\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.9622582657517156\n",
      "Dev Accuracy :  0.940149625935162\n",
      "Train Mean loss :  0.005741683148224432\n",
      "Dev Mean loss :  0.009807105199230644\n",
      "----------------------------------------\n",
      "EPOCH 8\n",
      "Train Accuracy :  0.9684965689332502\n",
      "Dev Accuracy :  0.8802992518703242\n",
      "Train Mean loss :  0.0049804061715921855\n",
      "Dev Mean loss :  0.013537599967602185\n",
      "----------------------------------------\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.9725514660012476\n",
      "Dev Accuracy :  0.9501246882793017\n",
      "Train Mean loss :  0.003766632957618499\n",
      "Dev Mean loss :  0.00822562213401842\n",
      "----------------------------------------\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.9541484716157205\n",
      "Dev Accuracy :  0.8827930174563591\n",
      "Train Mean loss :  0.004916869287398385\n",
      "Dev Mean loss :  0.02343615399037216\n",
      "----------------------------------------\n",
      "EPOCH 11\n",
      "Train Accuracy :  0.9444791016843419\n",
      "Dev Accuracy :  0.7805486284289277\n",
      "Train Mean loss :  0.0068548413294120215\n",
      "Dev Mean loss :  0.17207603621066658\n",
      "----------------------------------------\n",
      "EPOCH 12\n",
      "Train Accuracy :  0.932626325639426\n",
      "Dev Accuracy :  0.942643391521197\n",
      "Train Mean loss :  0.017212507087521214\n",
      "Dev Mean loss :  0.01299225689467052\n",
      "----------------------------------------\n",
      "EPOCH 13\n",
      "Train Accuracy :  0.9800374298190893\n",
      "Dev Accuracy :  0.9326683291770573\n",
      "Train Mean loss :  0.0028055766683175156\n",
      "Dev Mean loss :  0.012602544206187612\n",
      "----------------------------------------\n",
      "EPOCH 14\n",
      "Train Accuracy :  0.9650655021834061\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.004955451238027272\n",
      "Dev Mean loss :  0.008204615436711504\n",
      "----------------------------------------\n",
      "EPOCH 15\n",
      "Train Accuracy :  0.9753587024329382\n",
      "Dev Accuracy :  0.7930174563591023\n",
      "Train Mean loss :  0.0032940900073191617\n",
      "Dev Mean loss :  0.023089690547334285\n",
      "----------------------------------------\n",
      "EPOCH 16\n",
      "Train Accuracy :  0.9513412351840299\n",
      "Dev Accuracy :  0.9027431421446384\n",
      "Train Mean loss :  0.006813111845726024\n",
      "Dev Mean loss :  0.017413272599032394\n",
      "----------------------------------------\n",
      "EPOCH 17\n",
      "Train Accuracy :  0.9619463505926388\n",
      "Dev Accuracy :  0.7605985037406484\n",
      "Train Mean loss :  0.011318641902156664\n",
      "Dev Mean loss :  0.11109502297684438\n",
      "----------------------------------------\n",
      "EPOCH 18\n",
      "Train Accuracy :  0.9669369931378665\n",
      "Dev Accuracy :  0.9226932668329177\n",
      "Train Mean loss :  0.009214298449356105\n",
      "Dev Mean loss :  0.01608886917184416\n",
      "----------------------------------------\n",
      "EPOCH 19\n",
      "Train Accuracy :  0.9472863381160325\n",
      "Dev Accuracy :  0.9576059850374065\n",
      "Train Mean loss :  0.008015841469123115\n",
      "Dev Mean loss :  0.00845371627438543\n",
      "----------------------------------------\n",
      "EPOCH 20\n",
      "Train Accuracy :  0.9775421085464754\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.0027013239413106204\n",
      "Dev Mean loss :  0.011486860767861554\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_cbow(l_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbow(model, test_data, test_labels):\n",
    "    acc = 0\n",
    "    test_mean_loss = 0\n",
    "    \n",
    "    for x, labels in test_loader:\n",
    "            \n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            test_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    acc+=1\n",
    "    print(\"Test accuracy : \", acc/len(test_data))\n",
    "    print(\"Test mean loss : \", test_mean_loss/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.8830845771144279\n",
      "Test mean loss :  0.05090465237252155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_cbow(l_cbow, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On plot les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_results(train_acc, dev_acc, train_loss, dev_loss, model_name):\n",
    "    xaxis = [i for i in range(1,21)]\n",
    "    acc_file_name = \"acc_\"+\"_\".join(model_name.split())+\".png\"\n",
    "    loss_file_name = \"loss_\"+\"_\".join(model_name.split())+\".png\"\n",
    "    \n",
    "    plt.plot(xaxis,train_acc, label= \"Train accuracy\")\n",
    "    plt.plot(xaxis,dev_acc, label=\"Dev accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy on train and dev set over 20 epochs for a \"+model_name)\n",
    "    plt.savefig(acc_file_name)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(train_loss, label= \"Train loss\")\n",
    "    plt.plot(dev_loss, label=\"Dev loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Mean loss on train and dev set over 20 epochs for a \"+model_name)\n",
    "    plt.savefig(loss_file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_train_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-07a7ba3dbb17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_train_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_train_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_dev_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_dev_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_train_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "plot_train_results(rnn_train_accuracies, rnn_dev_accuracies, rnn_train_losses, rnn_dev_losses, \"GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un classifieur RNN classique (GRU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_classifier(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, vocab_size, embedding_dim, rnn_dropout, bidirectional=False):\n",
    "        super(GRU_classifier, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout,bidirectional=bidirectional)\n",
    "\n",
    "        if bidirectional:    \n",
    "            self.fc = nn.Linear(2*hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.is_bidirectional = bidirectional \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "            \n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(0)\n",
    "        embeds = self.embedding(inputs)\n",
    "        rnn_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        out = self.dropout(rnn_out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = th.sigmoid(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_bidirectional:\n",
    "            hidden = weight.new(2*self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La même classe mais avec de l'attention\n",
    "on implémente une version simplifiée de l'attention présentée dans ce papier : https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_with_attention_classifier(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, vocab_size, embedding_dim, rnn_dropout, bidirectional=False):\n",
    "        super(GRU_with_attention_classifier, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout,bidirectional=bidirectional)\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.is_bidirectional = bidirectional \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        #nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "        if bidirectional:    \n",
    "            self.fc_attn1 = nn.Linear(2*hidden_size, 100)\n",
    "        else:\n",
    "            self.fc_attn1 = nn.Linear(hidden_size, 100)\n",
    "            \n",
    "        self.fc_attn2 = nn.Linear(100,1, bias=False)    \n",
    "        self.fc_attn3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(0)\n",
    "        embeds = self.embedding(inputs)\n",
    "        rnn_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        out = self.dropout(rnn_out)\n",
    "        attention_weight = self.fc_attn1(out)\n",
    "        attention_weight = self.tanh(attention_weight)\n",
    "        attention_weight = self.softmax(self.fc_attn2(attention_weight))\n",
    "        out = (out*attention_weight).sum(dim=1)\n",
    "        out = self.fc_attn3(out)\n",
    "        out = th.sigmoid(out)\n",
    "        \n",
    "        return out, hidden, attention_weight\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_bidirectional:\n",
    "            hidden = weight.new(2*self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyper-paramètres\n",
    "nb_cells = 1\n",
    "hidden_size = 15\n",
    "embedding_dim = 10\n",
    "learning_rate = 1e-2\n",
    "loss_fn = nn.BCELoss()\n",
    "rnn_attention = GRU_with_attention_classifier(nb_cells, hidden_size, vocab_size, embedding_dim, 0.0, bidirectional=False)\n",
    "rnn_no_attention = GRU_classifier(nb_cells, hidden_size, vocab_size, embedding_dim, 0.0, bidirectional=False)\n",
    "\n",
    "rnn_train_accuracies = []\n",
    "rnn_train_losses = []\n",
    "rnn_dev_accuracies = []\n",
    "rnn_dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, batch_size,lr, max_epochs=20, conv=False):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    best_dict = model.state_dict()\n",
    "    best_acc = 0\n",
    "    for i in range(max_epochs):\n",
    "        train_accuracy = 0\n",
    "        dev_accuracy = 0\n",
    "        train_mean_loss = 0\n",
    "        dev_mean_loss = 0\n",
    "\n",
    "        n = 0 \n",
    "\n",
    "        for x, labels in train_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            h = h.data\n",
    "            \n",
    "            n+=loc_batch_size\n",
    "            \n",
    "            preds, _ = model(x, h)  \n",
    "            loss = loss_fn(preds, labels)\n",
    "            h = h.detach()\n",
    "            train_mean_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                    train_accuracy+=1\n",
    "                \n",
    "        for x, labels in dev_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            preds, _ = model(x, h)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            dev_mean_loss+=loss.item()\n",
    "            \n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((label==1 and pred>0.5) or(label==0 and pred<0.5)):\n",
    "                    dev_accuracy+=1\n",
    "        \n",
    "        if(i==0):\n",
    "            best_dict=model.state_dict()\n",
    "            best_acc=dev_accuracy/len(dev_data)\n",
    "        else:\n",
    "            if(best_acc<(dev_accuracy/len(dev_data))):\n",
    "                best_acc=dev_accuracy/len(dev_data)\n",
    "                best_dict=model.state_dict()\n",
    "                print(\"new best acc\")\n",
    "        \n",
    "        rnn_train_accuracies.append(train_accuracy/len(train_data))\n",
    "        rnn_train_losses.append(train_mean_loss/len(train_data))\n",
    "        rnn_dev_accuracies.append(dev_accuracy/len(dev_data))\n",
    "        rnn_dev_losses.append(dev_mean_loss/len(dev_data))\n",
    "        \n",
    "        print(\"EPOCH {}\".format(i+1))\n",
    "        print(\"Train Accuracy : \",train_accuracy/len(train_data))\n",
    "        print(\"Dev Accuracy : \",dev_accuracy/len(dev_data))\n",
    "        print(\"Train Mean loss : \",train_mean_loss/len(train_data))\n",
    "        print(\"Dev Mean loss : \",dev_mean_loss/len(dev_data))\n",
    "        print(\"----------------------------------------\")\n",
    "        \n",
    "#     model.load_state_dict(best_dict)\n",
    "#     acc = test(model, test_data, conv=conv)\n",
    "#     stat_dict[model_name][4].append(acc)\n",
    "#     print(\"Accuracy on test data : \", acc)\n",
    "#     return best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.6459762944479102\n",
      "Dev Accuracy :  0.7456359102244389\n",
      "Train Mean loss :  0.012093759603747263\n",
      "Dev Mean loss :  0.010979968339130468\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.8119151590767312\n",
      "Dev Accuracy :  0.8428927680798005\n",
      "Train Mean loss :  0.008221731026769057\n",
      "Dev Mean loss :  0.007564803534315114\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.9151590767311292\n",
      "Dev Accuracy :  0.8553615960099751\n",
      "Train Mean loss :  0.0048387576770722975\n",
      "Dev Mean loss :  0.007966723915496075\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.9666250779787897\n",
      "Dev Accuracy :  0.8678304239401496\n",
      "Train Mean loss :  0.002127841810439612\n",
      "Dev Mean loss :  0.012598053364087815\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 5\n",
      "Train Accuracy :  0.9806612601372426\n",
      "Dev Accuracy :  0.940149625935162\n",
      "Train Mean loss :  0.001242054601653676\n",
      "Dev Mean loss :  0.004491544325509111\n",
      "----------------------------------------\n",
      "EPOCH 6\n",
      "Train Accuracy :  0.9903306300686213\n",
      "Dev Accuracy :  0.9177057356608479\n",
      "Train Mean loss :  0.0006863926651353668\n",
      "Dev Mean loss :  0.005069646255073702\n",
      "----------------------------------------\n",
      "EPOCH 7\n",
      "Train Accuracy :  0.9934497816593887\n",
      "Dev Accuracy :  0.9326683291770573\n",
      "Train Mean loss :  0.000541166241284511\n",
      "Dev Mean loss :  0.005375154154140623\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 8\n",
      "Train Accuracy :  0.9903306300686213\n",
      "Dev Accuracy :  0.942643391521197\n",
      "Train Mean loss :  0.0005203146172724594\n",
      "Dev Mean loss :  0.004689387803915551\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 9\n",
      "Train Accuracy :  0.9956331877729258\n",
      "Dev Accuracy :  0.9451371571072319\n",
      "Train Mean loss :  0.00030911027301366937\n",
      "Dev Mean loss :  0.004163238804170774\n",
      "----------------------------------------\n",
      "EPOCH 10\n",
      "Train Accuracy :  0.9931378665003119\n",
      "Dev Accuracy :  0.9276807980049875\n",
      "Train Mean loss :  0.0004277064706677024\n",
      "Dev Mean loss :  0.00483228132056718\n",
      "----------------------------------------\n",
      "EPOCH 11\n",
      "Train Accuracy :  0.9937616968184654\n",
      "Dev Accuracy :  0.9177057356608479\n",
      "Train Mean loss :  0.0004827900786700537\n",
      "Dev Mean loss :  0.006815231946341117\n",
      "----------------------------------------\n",
      "EPOCH 12\n",
      "Train Accuracy :  0.9875233936369308\n",
      "Dev Accuracy :  0.9351620947630923\n",
      "Train Mean loss :  0.0007348948205928408\n",
      "Dev Mean loss :  0.004555515176210172\n",
      "----------------------------------------\n",
      "EPOCH 13\n",
      "Train Accuracy :  0.9968808484092326\n",
      "Dev Accuracy :  0.9451371571072319\n",
      "Train Mean loss :  0.00030770943638013415\n",
      "Dev Mean loss :  0.004442237456912115\n",
      "----------------------------------------\n",
      "EPOCH 14\n",
      "Train Accuracy :  0.9953212726138491\n",
      "Dev Accuracy :  0.9376558603491272\n",
      "Train Mean loss :  0.00027640943855706065\n",
      "Dev Mean loss :  0.004836143814316215\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 15\n",
      "Train Accuracy :  0.9984404242046163\n",
      "Dev Accuracy :  0.9476309226932669\n",
      "Train Mean loss :  0.0001638287630102685\n",
      "Dev Mean loss :  0.004599536102960926\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 16\n",
      "Train Accuracy :  0.9975046787273861\n",
      "Dev Accuracy :  0.9551122194513716\n",
      "Train Mean loss :  0.00016579996772566246\n",
      "Dev Mean loss :  0.004447222603621435\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 17\n",
      "Train Accuracy :  0.9931378665003119\n",
      "Dev Accuracy :  0.9576059850374065\n",
      "Train Mean loss :  0.0004276102021275479\n",
      "Dev Mean loss :  0.003243922620565814\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 18\n",
      "Train Accuracy :  0.9975046787273861\n",
      "Dev Accuracy :  0.970074812967581\n",
      "Train Mean loss :  0.0001880433057637912\n",
      "Dev Mean loss :  0.0032059825591608288\n",
      "----------------------------------------\n",
      "EPOCH 19\n",
      "Train Accuracy :  0.9965689332501559\n",
      "Dev Accuracy :  0.9650872817955112\n",
      "Train Mean loss :  0.00018158383851551207\n",
      "Dev Mean loss :  0.003339500429149637\n",
      "----------------------------------------\n",
      "EPOCH 20\n",
      "Train Accuracy :  0.9984404242046163\n",
      "Dev Accuracy :  0.9675810473815462\n",
      "Train Mean loss :  0.00012093148687656874\n",
      "Dev Mean loss :  0.003045389797613129\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_rnn(rnn_no_attention, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Accuracy :  0.6912039925140362\n",
      "Dev Accuracy :  0.8553615960099751\n",
      "Train Mean loss :  0.01166522942125537\n",
      "Dev Mean loss :  0.008411078233076748\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 2\n",
      "Train Accuracy :  0.9008109794135996\n",
      "Dev Accuracy :  0.9526184538653366\n",
      "Train Mean loss :  0.005058004011307667\n",
      "Dev Mean loss :  0.0029274715932824666\n",
      "----------------------------------------\n",
      "new best acc\n",
      "EPOCH 3\n",
      "Train Accuracy :  0.9775421085464754\n",
      "Dev Accuracy :  0.9775561097256857\n",
      "Train Mean loss :  0.0012656805087787263\n",
      "Dev Mean loss :  0.0014862030642362604\n",
      "----------------------------------------\n",
      "EPOCH 4\n",
      "Train Accuracy :  0.9928259513412352\n",
      "Dev Accuracy :  0.972568578553616\n",
      "Train Mean loss :  0.0005367599983341732\n",
      "Dev Mean loss :  0.0017374053683382764\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2f7508743c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_attention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-6713eb6129fd>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[1;34m(model, batch_size, lr, max_epochs, conv)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_mean_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rnn(rnn_attention, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn(model, test_data, test_labels):\n",
    "    acc = 0\n",
    "    test_mean_loss = 0\n",
    "    \n",
    "    for x, labels in test_loader:\n",
    "            loc_batch_size = x.size(0)\n",
    "            h = model.init_hidden(loc_batch_size)\n",
    "            preds, _ = model(x, h)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            test_mean_loss+=loss.item()\n",
    "            for pred, label in zip(preds, labels):\n",
    "                if((pred<0.5 and label==0) or (pred>0.5 and label == 1)):\n",
    "                    acc+=1\n",
    "    print(\"Test accuracy : \", acc/len(test_data))\n",
    "    print(\"Test mean loss : \", test_mean_loss/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.9776119402985075\n",
      "Test mean loss :  0.0021201462075994605\n"
     ]
    }
   ],
   "source": [
    "test_rnn(rnn_no_attention, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified = 10*[0]\n",
    "modified[0]=\"i think all that strife , it kind of helped me and both break down the barrier of caring what people thought about what we were doing , , mr ivey said the couple had twins , then lost one to a rare heart in 2010 ms price dealt with the loss , along with feelings of professional failure , by self with alcohol over the years , she and mr ivey a house s worth of prized gear and personal possessions to buy food , cover touring expenses and record midwest farmer s daughter at sun studio in memphis it kind of felt like there was the symbolism of it we are willing to put up the car , the wedding ring , she said we would rather have this album exist out here , even if nothing would have happened to it maybe 20 , 30 years down the road , or after i am dead , somebody would find it and it would become a cult thing like nick drake , you know \\? photo things could have gone that way several indie and at least one major country label passed on \"\n",
    "modified[1]=\"paris some of the greatest works of pablo picasso go on display in paris this week , in an exhibition that charts a pivotal year in which he cemented his status as a titan of 20th century art picasso 1932 erotic year , chronicles the spanish artist s prolific creativity over those 12 months , referred to in the art world as his year of wonders picasso produced more than 300 works of art in 1932 and the exhibition at paris musee picasso features over 100 of them they include the masterpiece the dream a highly portrait of a sleeping marie walter , who was the artist s mistress at the time another highlight is a girl before a mirror , which also depicts walter we called 1932 an erotic year because picasso used as a driving force in his painting , exhibition curator told reuters the daughter of spanish painter pablo picasso , maya picasso is seen in front of her father 's paintings during a visit by french emmanuel macron \\( not pictured \\) at the picasso 1932 erotic year exhibition at the picasso museum in paris , france , october\"\n",
    "modified[2]=\"french president emmanuel macron delivers his speech during a meeting at international food market in , near paris , france , october 11 , 2017 francois mori pool paris president emmanuel macron called for changes to france s food chain on wednesday to ensure that farmers , who have been hit by squeezed margins and a retail price war , are paid fairly macron said he supported a new type of contract , based on farmers production costs , which would require stronger producer organizations and a change in legislation the changes are part of a wide field to fork review promised by macron during his presidential campaign as he sought to appease farmers , an important constituency in french politics , who have also complained of excessive red tape last year a third of farmers earned less than 350 euros \\( 403 \\) a month , the agricultural mutual assistance association \\( \\) said , a third of the net minimum wage macron endorsed a proposal from the workshops to create a reversed contract starting from farmers , to food processors and to retailers this would ensure a better spread of added \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = 10*[0]\n",
    "original[0] = \"i think all that strife , it kind of helped me and both break down the barrier of caring what people thought about what we were doing , , mr ivey said the couple had twins , then lost one to a rare heart in 2010 ms price dealt with the loss , along with feelings of professional failure , by self with alcohol over the years , she and mr ivey a house s worth of prized gear and personal possessions to buy food , cover touring expenses and record midwest farmer s daughter at sun studio in memphis it kind of felt like there was the symbolism of it we re willing to put up the car , the wedding ring , she said we would rather have this album exist out here , even if nothing would have happened to it maybe 20 , 30 years down the road , or after i m dead , somebody would find it and it would become a cult thing like nick drake , you know \\? photo things could have gone that way several indie and at least one major country label passed on \"\n",
    "original[1]=\"paris \\( reuters \\) some of pablo picasso s greatest works go on display in paris this week , in an exhibition that charts a pivotal year in which he cemented his status as a titan of 20th century art picasso 1932 erotic year , chronicles the spanish artist s prolific creativity over those 12 months , referred to in the art world as his year of wonders picasso produced more than 300 works of art in 1932 and the exhibition at paris musee picasso features over 100 of them they include the masterpiece the dream a highly portrait of a sleeping marie walter , who was the artist s mistress at the time another highlight is girl before a mirror , which also depicts walter we called 1932 an erotic year because picasso used as a driving force in his painting , exhibition curator told reuters the daughter of spanish painter pablo picasso , maya picasso is seen in front of her father 's paintings during a visit by french president emmanuel macron \\( not pictured \\) at the picasso 1932 erotic year exhibition at the picasso museum in paris , france , october\"\n",
    "original[2]=\"french president emmanuel macron delivers his speech during a meeting at international food market in , near paris , france , october 11 , 2017 francois mori pool paris president emmanuel macron called for changes to france s food chain on wednesday to ensure that farmers , who have been hit by squeezed margins and a retail price war , are paid fairly macron said he supported a new type of contract , based on farmers production costs , which would require stronger producer organizations and a change in legislation the changes are part of a wide field to fork review promised by macron during his presidential campaign as he sought to appease farmers , an important constituency in french politics , who have also complained of excessive red tape last year a third of farmers earned less than 350 euros \\( 403 \\) a month , the agricultural mutual assistance association \\( \\) said , a third of the net minimum wage macron endorsed a proposal from the workshops to create a reversed contract starting from farmers , to food processors and to retailers this would ensure a better spread of added \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_infer(model, news):\n",
    "    model.eval()\n",
    "    idx = th.Tensor([word2idx[w] for w in news.split()]).long().view(1,-1)\n",
    "    h0 = model.init_hidden(1)\n",
    "    out,_=model(idx, h0)\n",
    "    if(out>0.5):\n",
    "        print(\"fake\")\n",
    "    else:\n",
    "        print(\"false\")\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_infer(model, news):\n",
    "    model.eval()\n",
    "    idx = th.Tensor([word2idx[w] for w in news.split()]).long().view(1,-1)\n",
    "    out=model(idx)\n",
    "    if(out>0.5):\n",
    "        print(\"fake\")\n",
    "    else:\n",
    "        print(\"true\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "cbow infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[1.]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[1.0000]], grad_fn=<SigmoidBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru sans attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([0.9998], grad_fn=<SelectBackward>)\n",
      "fake\n",
      "modifié :  tensor([0.9998], grad_fn=<SelectBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru avec attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[0.9404]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[0.8930]], grad_fn=<SigmoidBackward>)\n",
      "--------------------\n",
      "\n",
      "\n",
      "1\n",
      "cbow infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[1.]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[1.]], grad_fn=<SigmoidBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru sans attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([1.0000], grad_fn=<SelectBackward>)\n",
      "fake\n",
      "modifié :  tensor([1.0000], grad_fn=<SelectBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru avec attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[0.9970]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[0.9967]], grad_fn=<SigmoidBackward>)\n",
      "--------------------\n",
      "\n",
      "\n",
      "2\n",
      "cbow infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[1.]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[1.]], grad_fn=<SigmoidBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru sans attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([1.0000], grad_fn=<SelectBackward>)\n",
      "fake\n",
      "modifié :  tensor([1.0000], grad_fn=<SelectBackward>)\n",
      "-------------------- \n",
      "\n",
      "gru avec attention infer, original puis modifié\n",
      "fake\n",
      "original :  tensor([[0.9967]], grad_fn=<SigmoidBackward>)\n",
      "fake\n",
      "modifié :  tensor([[0.9967]], grad_fn=<SigmoidBackward>)\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)\n",
    "    print(\"cbow infer, original puis modifié\")\n",
    "    print(\"original : \",cbow_infer(l_cbow, original[i]))\n",
    "    print(\"modifié : \",cbow_infer(l_cbow, modified[i]))\n",
    "    print(20*\"-\",'\\n')\n",
    "\n",
    "    print(\"gru sans attention infer, original puis modifié\")\n",
    "    print(\"original : \",gru_infer(rnn_no_attention, original[i]))\n",
    "    print(\"modifié : \",gru_infer(rnn_no_attention, modified[i]))\n",
    "    print(20*\"-\",'\\n')\n",
    "\n",
    "    print(\"gru avec attention infer, original puis modifié\")\n",
    "    print(\"original : \",gru_infer(rnn_attention, original[i]))\n",
    "    print(\"modifié : \",gru_infer(rnn_attention, modified[i]))\n",
    "    print(\"-\"*20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
